[
["index.html", "Machine Learning Summary License", " Machine Learning 34142/1916/2021/1/38David D’Haese Gepubliceerd op 2020-10-14Juncus effusus L., Motic SMZ171© 2020 David D’Haese Summary Machine learning course of the AP University College at Antwerp, Belgium. The course is in Dutch. License The content of this project including its source code is licensed under the GNU Affero General Public v3.0 license. You are allowed to… Under the conditions… You are not allowed to… Commercial use Disclose source Liability Distribution License and copyright notice Warranty Modification Network use is distribution Patent use Same license Private use State changes "],
["inleiding-tot-de-cursus.html", "Hoofdstuk 1 Inleiding tot de cursus 1.1 In een notedop 1.2 Leerdoelen 1.3 Cursus vorm 1.4 Bekijken van deze Cursus 1.5 Code uit de cursus uitvoeren 1.6 Oefeningen maken 1.7 Licentie voor deze cursus 1.8 Verwijzen naar deze cursus", " Hoofdstuk 1 Inleiding tot de cursus 1.1 In een notedop In deze cursus zullen we het erg breed domein van machinaal leren (ML; eng: machine learning) aansnijden. Omdat de discipline zo breed is, kunnen onmogelijk alle topics aan bod komen. In plaats daarvan is er gekozen om eerst een brede basis te verschaffen rond de begrippen en principes die belangrijk zijn voor ML om ons daarna specifiek te richten op deep learning. We gaan leren hoe computers beelden kunnen herkennen en gaan zelf aan de slag om zulke algoritmes te configureren en te gebruiken. Deze aangeleerde vaardigheid maakt van een doorsnee IT-er een beginnende datawetenschapper. Gaandeweg zullen we ons echter van bewust moeten maken dat deze vaardigheid ook een aantal verantwoordelijkheden met zich meebrengt. Het gaat dan voornamelijk om ethisch ML en ethische artificiële intelligentie (AI). 1.2 Leerdoelen Hieronder, in Tabel 1.1, staan de leerdoelen opgesomd zoals ze vermeld staan in de ECTS fiches voor dit opleidingsonderdeel. In de cursus zal er naar deze leerdoelen verwezen worden met vermelding van de bijhorende code. Tabel 1.1: Leerdoelen voor deze cursus Code Omschrijving EA_LD751 Begrijpt de basis principes van machine learning EA_LD752 Herkent de verschillen tussen supervised, unsupervised en reinforcement learning EA_LD753 Begrijpt de fundamenten achter deep learning EA_LD754 Onderkent de basisprincipes van data training &amp; Cross-validatie EA_LD755 Ontwikkelt het vermogen om zelfstandig technisch-uitdagende online workshops uit te voeren EA_LD756 Analyseert zelfstandig een probleemstelling met het oog op het bieden van een AI-gerelateerde oplossing EA_LD757 Ontwikkelt de correcte AI strategie op basis van een probleemstelling EA_LD758 Past de juiste principes toe tijdens het exploreren, hanteren en opkuisen van data EA_LD759 Herkent de risico’s van onvolledige en inaccurate data EA_LD760 Gebruikt een diagnostische toolset om de performantie van ML modellen te meten EA_LD761 Beeld complexe data uit door middel van hedendaagse visualisatie tools EA_LD762 Evalueert op gepaste wijze de performantie van een algoritme. EA_LD763 Rapporteert op correcte wijze de resultaten van een ML analyse 1.3 Cursus vorm Deze cursus is geschreven in een versie van Markdown. Markdown is een familie van zogenaamde opmaaktalen (eng: mark-up languages) die ervoor zorgen dat inhoud van het document in verscheidene formaten weergegeven kan worden: PDF, HTML, …. Het loskoppelen van inhoud betekent enerzijds dat de auteur zich kan focusseren op de inhoud in plaats van de vorm. Anderzijds betekent het dat de lezer in staat is zijn de vorm van de uitvoer te bepalen, bijvoorbeeld, beter leesbaarheid, donkere achtergrond, …. Voor meer technische documenten biedt Markdown nog veel belangrijkere voordelen. Het maakt het mogelijk om code in de ene of andere taal tussen de lyrische tekst te plaatsen en uit te voeren. Met de juiste IDE (Integrated Development Environment), betekent dit dat de auteur én de lezer in staat zijn om in meerdere programmeertalen tegelijkertijd te werken! Figuur 1.1: Werking van Markdown. De platte tekst (links) wordt omgezet naar een ander formaat (rechts; hier HTML) door een externe tool als Pandoc. Stijl-regels worden hier automatisch uitgevoerd maar de auteur heeft de mogelijkheid ook deze in detail te configureren. Naast het scheiden van vormgeving en inhoud (hetgeen een merkelijke efficiëntie verbetering met zich meebrengt) ondersteund R Markdown ook meertaligheid, i.e. meerdere programmeertalen in één document. Tussen een aantal talen is er zelfs wederzijdse uitwisseling mogelijk van de actieve variabelen (zie oranje markeringen met pijl). Het voorbeeld met de Mandelbrot fractaal is afkomstig van Li (2017), waarvan de GitHub repository een bondige beschrijving geeft van de Mandelbrot verzameling (eng: Mandelbrot set) met een test die de performantie-winst van Julia t.o.v. R onderzoekt. 1.4 Bekijken van deze Cursus Deze cursus kan je het best bekijken door gebruik te maken van je browser. Voor een betere ervaring zijn je de browser het best op volledig scherm. Er is een sneltoets om wisselen tussen gewoon en volledig-scherm modus (Chrome, Firefox: F11, Safari: Control + ⌘ + F). Heb je meerdere tabs open en vind je het vervelend navigeren in volledig-scherm-modus, is er voor Chrome een leuke extension: QuicKey.. 1.5 Code uit de cursus uitvoeren Via deze github repository krijg je toegang tot de broncode van deze cursus. Opgelet: De inhoud van de cursus kan mogelijk nog wijzigen. Om steeds de laatste versie te hebben maak je gebruik van git clone. Je kan met deze broncode alle code blokken uit de RMarkdown-bestanden (*.Rmd) rechtstreeks lokaal uitvoeren. Maar opgelet, sommige code-blokken zijn specifiek voor Bookdown bedoeld en zijn er alleen voor de vormgeving van de cursus. Let er ook op dat je alle variabelen en bibliotheken in scope hebt door de nodige bovenliggende code-blokken eerst uit te voeren. 1.6 Oefeningen maken Her-en-der verspreid in de cursus zullen er oefeningen voorzien worden. Deze zou je ook rechtstreeks in de broncode van de cursus moeten kunnen uitvoeren. Wanneer je tevreden bent van je oefening, kan je deze via Digitap indienen. 1.7 Licentie voor deze cursus De inhoud van deze cursus valt onder een GNU Affero General Public v3.0 licentie. Wat er toegelaten is en onder welke voorwaarden staat hieronder opgesomd: Je mag… Onder voorwaarde dat… Je mag niet… Commercieel gebruik Ontsluit bron Aansprakelijk stellen Verspreiden Licentie en copyright notice mee verspreiden Garantie Aanpassen Netwerk verspreiding Patenteren Zelfde licentie Privé gebruiken Bekendmaking wijzigingen 1.8 Verwijzen naar deze cursus Bibtex-formaat: @online{dhaese2020machine-learning, author = {D’Haese, David}, title = “Machine Learning”, year = “2020”, url = “https://ddhaese.github.io/machine-learning/”, note = \"[Online; accessed } APA-formaat: D’Haese, D., 2020. Machine learning [WWW Document] [Online; accessed yyyy-mm-dd]. URL https://ddhaese.github.io/machine-learning/ Bronvermelding "],
["leren-uit-data.html", "Hoofdstuk 2 Leren uit data 2.1 Het leerproces 2.2 De evolutie van het machinaal leren 2.3 Intelligentie 2.4 Het model 2.5 Doelfunctie 2.6 MNIST dataset 2.7 Het resultaat van MNIST analyse 2.8 Het MNIST model 2.9 Het leerproces voor begeleid ML 2.10 De onderdelen van een model 2.11 Hyperparameters 2.12 Het leeralgoritme 2.13 Model complexiteit 2.14 Comprimeren door middel van een ML model 2.15 Leren versus ontwerp 2.16 Leren versus onthouden en inferentie 2.17 Onbegeleid ML 2.18 Conditionering", " Hoofdstuk 2 Leren uit data 2.1 Het leerproces Leerdoel 2.1 Begrijpt de basisprincipes van machine learning (EA_LD751). Leerdoel 2.2 Herkent de verschillen tussen supervised, unsupervised en reinforcement learning (EA_LD752). Leerdoel 2.3 Ontwikkelt de correcte AI strategie op basis van een probleemstelling (EA_LD757). Machine learning (ML) is het vermogen van een algoritme om te leren uit data. In deze inleiding zullen we leren dat, hoewel dit een eenvoudige definitie lijkt, hier toch héél wat achter schuilt. Dus om deze definitie wat kracht bij te zetten, maken we hier meteen ons eerst kadertje voor: Stelling 2.1 Machine learning (ML) is het vermogen van een algoritme om te leren uit data. De term word ook ruimer geïnterpreteerd als de discipline die zich bezighoud met het creëren van zulke algoritmen. Laten we eens filosoferen over de term leren in deze stelling. Wat betekent het in feite om iets te leren? Prof. Abu-Mostafa, een gerenommeerde didacticus in deze discipline legt het in zijn boek Learning from data (Abu-Mostafa et al. (2012), p.1) ongeveer zo uit: Laat een foto aan een driejarige zien en vraag de kleuter of er een boom te zien is en je krijgt bijna zeker het juiste antwoord. Vraag nu aan een dertig-jarige persoon om de definitie van een boom te geven en je krijgt vermoedelijk een onduidelijk of toch onvolledig antwoord. Hoe komt dit? Een mens leert niet wat een boom is door de wiskundige beschrijving van een boom te memoriseren maar door het beeld te associëren met het woord en de klank en door gecorrigeerd te worden telkens wanneer we een fout maken. De foto’s en tekeningen, de geur, het geritsel van de bladeren, de juf die ‘boom’ zegt, de ouder die wijst, de vier letters op het schoolbord: dat zijn de data. Het verzamelen van al die data en classificeren ervan onder de noemer ‘boom’ gebeurt door onze hersenen en dat proces noemen we leren. Laten we dit schematisch voorstellen: Figuur 2.1: Het centraal zenuwstelsel van een mens kan leren uit data. Het concept ‘boom’ wordt hier met een ‘gedachten-wolk’ voorgesteld en komt overeen met een model van de realiteit. Het eindpunt van het leerproces is het model. In het voorbeeld van de boom is dit model het concept of mentaal ‘beeld’ van een boom dat in onze hersenen achterblijft ook wanneer we niet naar een boom kijken. Dit model is een vereenvoudigde versie van de werkelijkheid. Het bevat zeker niet alle wiskundige verhoudingen noch alle biologische details van één boom, laat staan van alle bomen op aarde. Toch slagen de meesten onder ons een onderscheid te maken tussen een boom en een niet-boom. Stelling 2.2 Het eindpunt van het leerproces is een model dat een vereenvoudigde versie van de werkelijkheid kan weergeven Persoonlijkheid 2.1 (Yaser Abu-Mostafa) Yaser Abu-Mostafa is een professor Computer Science aan de California Institute of Technology. Hij is beroemd geworden voor zijn memorabele online cursussen rond ML. Zier hier for meer details. 2.2 De evolutie van het machinaal leren De mens is al eeuwenlang bezig met het bouwen van automaten (zogenaamde automata). Een prachtig voorbeeld hiervan is de geautomatisserde tekenaar-schrijver die de Zwitserse mechanieker Henri Maillardet bouwde rond 1800: Hierbij ging het meestal om voorgeprogrammeerde robotten zonder enige vorm van intelligentie. Eén van de eerste vormen van meer ‘intelligente’ robotten waren de machina speculatrix van Grey Walters: Ondertussen weten we dat er zelf-rijdende wagens, zelf-vliegende drones en zelf-varende boten bestaan: 2.3 Intelligentie ML is een onderdeel van artificiële intelligentie (AI). Er bestaan vele definities voor deze term, maar het VLEVA (Vlaams–Europees Verbindingsagentschap) maakt een duidelijk onderscheid. Zij spreken van ML wanneer patronen in data, al dan niet rechtstreeks afkomstig van sensoren, worden omgezet naar een model, zoals eerder reeds aangehaald. AI gaat een stap verder. Hier wordt er, op basis van het model en nieuwe data, werkelijk ook actie ondernomen en keuzes gemaakt. Bij ML wordt deze fase nog aan de mens overgelaten. Dus zelf-rijdende wagens behoren duidelijk tot het domein van AI, terwijl een applicatie die op basis van een paar fluittonen een melodie kan herkennen eerder thuishoort onder ML. Andere voorbeelden van ML toepassingen zijn het voorspellen van de financiële markten, beeldherkenning, geautomatiseerde medische diagnoses, enz… Dit onderscheid blijft natuurlijk erg artificieel en in sommige gevallen zal deze classificatie niet opgaan. Maar we kunnen wel zeggen: Stelling 2.3 De huidige vormen van artifiële intelligentie baseren bijna uitsluitend op machinaal aangeleerde modellen Dus geen AI zonder ML. Maar wat betekent intelligentie in feite? Volgens Wikipedia bestaat intelligentie uit meerdere capaciteiten zoals (logisch) redeneren, begrijpen, zelfbewustzijn, aanleren, emotionele intelligentie, plannen, creatief zijn, kritisch denken en probleemoplossing. Voorlopig ligt de focus bij ML voornamelijk op het aanleren al wordt ook op alle andere aspecten heel wat onderzoek verricht. We moeten voorlopig dus best nog bescheiden blijven met het gebruik van de term artificiële intelligentie. Het is ook heel belangrijk om te realiseren dat er vormen van intelligentie bestaan die erg verschillen van de menselijke intelligentie. Uiteraard wist je dat vele andere dieren aspecten van intelligentie kunnen bezitten. maar wist je ook dat er bij planten vormen van intelligentie werden vastgesteld en dit maakt het des te merkwaardiger gezien zij niet over een centraal zenuwstelsel beschikken. 2.4 Het model Laten we beginnen met het eindproduct van ML. We zagen dat het eindpunt van ML het model is. In tegenstelling tot wat deze term in bijvoorbeeld database management betekent, kan je het model het best beschouwen als een functie \\(\\hat{f}\\), ook wel de finale hypothese genoemd (final hypothesis; Abu-Mostafa et al. (2012)): \\[\\hat{f}: \\mathcal{X} \\to \\mathcal{Y}\\] Proberen we nu ons boom-herkenning oefening te vertalen van de menselijke wereld naar de ML wereld, dan bestaat het model uit een functie die een antwoord biedt op de vraag: Wordt op deze digitale foto een boom afgebeeld? Nadat de digitale afbeelding een zekere voorbereiding (eng: pre-processing) doorlopen heeft, zal het bestaan uit mogelijk een set van variabelen zoals voorgesteld in figuur 2.2. Figuur 2.2: Als we de oefening om een boom te herkennen uit figuur 2.1 vertalen naar de machinale wereld krijgen we data van waaruit het algoritme een model heeft aangeleerd. Het model kunnen we hier voorstellen door middel van een functie \\(\\hat{f}\\). Het hoedje op de \\(f\\) dient om aan te geven dat het om een geschatte waarde gaat. \\(\\mathcal{X}\\) is hier de input en bestaat in dit voorbeeld uit allerhande variabelen die betrekking hebben op gedigitaliseerde afbeeldingen. Bijvoorbeeld, de variabele Dominante kleur stelt de meest frequente kleur-groep die op de afbeelding voorkomt. \\(\\mathcal{Y}\\) is de output, de variabele die het antwoord bevat op de vraag of er al dan niet een boom wordt afgebeeld waarbij de witte en zwarte bollen overeenkomen met ja/neen of true/false. In python zal het model dus ongeveer de volgende vereenvoudigde vorm kunnen aannemen: def model(x): # Some code goes here return y en het equivalent in R is als volgt: model &lt;- function(x_test){ # Some code goes here return(y_predicted) } 2.5 Doelfunctie Het is je misschien opgevallen dat het symbool voor het model een hoedje draagt: \\(\\hat{f}\\). Dat is geen toeval. Dat wil in feite zeggen dat het om een geschatte functie gaat. Daartegenover staat de werkelijke functie \\(f\\), de zogenaamde doelfunctie (Abu-Mostafa et al. (2012)). Het is belangrijk om te beseffen dat de doelfunctie \\(f\\) in de meeste gevallen onbekend blijft! Tijdens simulaties of binnen de wetenschappelijke disciplines gebeurt het wel eens dat de \\(f\\) gekend is, maar in de meeste gevallen waar een datawetenschapper mee te maken krijgt is dat niet het geval. Het doel is uiteraard om te proberen om met het model \\(\\hat{f}\\) de doelfunctie \\(f\\) zo goed mogelijk te benaderen, en dat valt niet mee als \\(f\\) onbekend is. Er zijn verscheidene opties om deze benadering tot een succes te maken. Één van deze benaderingen, diegene die we in dit hoofdstuk toepassen, noemt men het begeleid leren (eng: supervised learning). Stelling 2.4 Het doel van begeleid ML is om een model \\(\\hat{f}\\) te vinden dat de werkelijke doelfunctie \\(f\\), die meestal onbekend blijft, zo goed mogelijk tracht te benaderen. 2.6 MNIST dataset We gaan nu trachten de eerste ideeën rond ML te vertalen naar de praktijk. Om dit mogelijk te maken, wordt er een nieuwe probleem met bijhorende dataset voorgesteld. Behalve het begrip ‘boom’ zullen de meeste peuters ook één of ander schrift aangeleerd krijgen. Hoewel gedrukte letters tegenwoordig natuurlijk domineren, wordt er nog steeds het handschrift aangeleerd (figuur 2.3). Behalve mensen is het natuurlijk interessant mocht een computer ook handgeschreven teksten kunnen lezen zodat bijvoorbeeld historische werken gedigitaliseerd kunnen worden. Figuur 2.3: Het handgeschreven alfabet zoals voorgesteld met de schrijfmethode D’Haese (geen familie; bron). Om dit mogelijk te maken kan men een model creëren dat in staat is om de handgeschreven karakters te herkennen. Het Amerikaanse Modified National Institute of Standards and Technology biedt een dataset aan met foto’s van handgeschreven letters en cijfers in allerlei varianten. Om de zaak te vereenvoudigen gaan we ons in dit voorbeeld beperken tot de handgeschreven cijfers (figuur 2.4). De cijfers (in de training set) zijn afkomstige van 250 schrijvers, zowel werknemers van de Census Bureau als studenten van het hoger middelbaar onderwijs. Figuur 2.4: Subset van de MNIST dataset met afbeeldingen van handgeschreven cijfers. Elke geschreven karakter is een afzonderlijke afbeelding van \\(28\\times28\\) pixels. Elke afbeeldingen werd uitgeknipt uit een groter gescand document met geschreven tekst, gecentreerd en teruggebracht naar een grootte van \\(28\\times28\\). Oefening 2.1 (MNIST probleem) Kijk eens goed naar de variaties voor de cijfers in Figuur 2.4. Valt je iets op? Zou er een probleem kunnen ontstaan indien je deze dataset bijvoorbeeld zou willen inzetten voor het leren herkennen van de handgeschreven notities van AP-studenten? Geef je antwoord op Digitap. Elke afbeelding in de MNIST dataset wordt voorgesteld als een \\(28\\times28\\)-matrix. Elke element van deze matrix stelt een grijswaarde voor van een pixel uit de afbeelding. Stel dat de afbeeldingen in de RGB kleurruimte zouden zijn opgeslagen, dan zou er voor elke pixel niet één maar drie waarden beschikbaar worden gemaakt. Figuur 2.5: Het 300ste cijfer het de MNIST training-set. Elke instantie van de input dataset (eng: instance) wordt dus voorgesteld door een \\(1\\times784\\)-vector en de ganse input dataset kan worden voorgesteld als een \\(n\\times784\\)-matrix, waarbij \\(n=60\\,000\\) de grootte van de (training-) dataset is zoals beschikbaar gemaakt door MNIST. 2.7 Het resultaat van MNIST analyse We gaan later in detail onderzoeken hoe we precies de beeldherkenning kunnen uitvoeren. Nu nemen we eerst een kortere weg naar het resultaat. Kijk even terug naar figuur 2.2 en herinner je dat het resultaat van een ML process een model was dat wiskundig als functie \\(\\hat{f}\\) kan worden voorgesteld. Schematisch zal onze analyse er nu uitzien zoals voorgesteld in figuur 2.6. Merk hier wel op dat de cijfer-varianten in werkelijkheid vervat zitten in een ‘platte’ matrix is met dimensies \\(60\\,000\\times784\\), een matrix dus met méér dan 47 miljoen zwevende kommagetallen (eng: floating point). Figuur 2.6: De schematische weergave van het leerproces voor het herkennen van handgeschreven cijfers uit de MNIST dataset. Stellen we het model voor met de Python functie model, kan kunnen we dit aanroepen met één of meerdere afbeeldingen van handgeschreven cijfers: y_prd_300 = model(x_trn[299:300]) Hier roepen we het model aan met de 300ste afbeelding uit de training-set (zie figuur 2.5), hier voorgesteld als x_trn[299:300]. Het resultaat van deze oproep wordt in variabele y_prd_300 bewaard. In wiskundige termen schrijven we \\(\\hat{y}_{300}\\), waarbij het hoedje aangeeft dat het niet de werkelijk \\(y\\)-waarde is maar de geschatte of voorspelde \\(y\\)-waarde. In theorie zou men verwachten dat de inhoud y_prd_300 gelijk is aan 6. Het is zeker mogelijk om model zo te schrijven dat dit inderdaad het geval is, maar meestal wordt er de voorkeur aan gegeven om naast de eigenlijke voorspelling ook een maat van de betrouwbaarheid van deze schatting mee te laten geven. In dit geval ziet y_prd_300 er zo uit: [[1.3147994e-08 4.3827520e-12 5.0856454e-07 1.3127689e-09 4.5237027e-07 5.3989115e-09 9.9999893e-01 5.3377733e-09 1.6693608e-07 3.1370115e-09]] Dit is iets heel anders dan 6, wat is hier gaande? In feite bestaat het model uit de kans dat de aangeleverde afbeelding overeenkomt met één van de 10 mogelijke uitkomsten, namelijk de cijfers 0 tot en met 9: Cijfer \\(p(instantie=cijfer|beeld)\\) 0 1.3147994e-08 1 4.3827520e-12 2 5.0856454e-07 3 1.3127689e-09 4 4.5237027e-07 5 5.3989115e-09 6 9.9999893e-01 7 5.3377733e-09 8 1.6693608e-07 9 3.1370115e-09 We zien nu ogenblikkelijk dat \\(p(\\hat{y}_{300}=6|x_{300}) = 99.99\\%\\). Dus in plaats dat het model ons het cijfer 6 teruggeeft, geeft het een schatting van de kansverdeling over alle mogelijke uitkomsten \\(P(cijfer|beeld)\\). Het is dan aan de datawetenschapper om, in samenspraak met anderen binnen het team, de drempelwaarde (eng: threshold) te bepalen tussen wat voldoende zeker is en wat niet. Stel dat we in dit geval de drempelwaarde \\(h_\\tau=0.99=99\\%\\) aannemen, dan vertaalt dit inderdaad naar het (in dit geval correcte) antwoord \\(\\hat{y}_{300}=\\hat{f}(x_{300}, h_\\tau=0.99)=6\\). 2.8 Het MNIST model We weten nog steeds niet hoe het model voor het herkennen van de MNIST cijfers eruit ziet, laat staan hoe het tot stand is gekomen. Laten we eerst eens proberen na te gaan hoe het model er van binnen uitziet. In dit geval bestaat het model uit een complex netwerk van gewichten \\(\\mathbf{\\theta}\\) die in lagen verdeeld zijn (zie figuur 2.7). Figuur 2.7: Schematische voorstelling van het MNIST model. Elke grijswaarde (uiterst links) in een afbeelding is verbonden met allerlei noden van de eerste laag in het netwerk (groot blauw raster links) en elke node uit de eerste laag is verbonden met de noden uit de tweede laag (kleiner blauw raster rechts). De oranje balk uiterst rechts stelt het antwoord voor, de kansverdeling over alle mogelijke uitkomsten, zoals uitgelegd in vorige paragraaf. We kunnen de gewichten uit het model opvragen: &lt;tf.Variable &#39;dense_4/kernel:0&#39; shape=(784, 128) dtype=float32, numpy= array([[ 0.08104221, -0.03245814, 0.05782456, ..., 0.01628028, 0.01527335, -0.0621725 ], [-0.02486849, -0.05942973, -0.05000503, ..., 0.05511766, 0.01111819, -0.01273195], ..., [-0.03103521, 0.04082336, -0.03615728, ..., -0.07942228, 0.05170608, -0.04771789], [-0.08011643, -0.02025594, 0.07396395, ..., -0.04995897, 0.04946151, 0.0736245 ]], dtype=float32)&gt; [[2]] &lt;tf.Variable &#39;dense_4/bias:0&#39; shape=(128,) dtype=float32, numpy= array([ 0.1288752 , 0.12100432, -0.00982326, 0.1232934 , 0.02084712, 0.01321915, -0.01993161, 0.0420486 , -0.08715703, 0.06092769, ..., 0.11226735, -0.06676487, -0.03609373, -0.05284352, -0.07687864, 0.16832131, 0.08192852, -0.07676063, 0.03645244, -0.04636759, 0.05661207, 0.13083224, 0.02666049], dtype=float32)&gt; In het totaal bestaat deze specifieke versie van het MNIST model dus uit \\(784\\times128 + 128 = 100\\,480\\) gewichten. Deze gewichten worden op een nog nader te begrijpen manier gebruikt om \\(\\hat{y}\\) te berekenen uit \\(x\\). Het model zou er dus ongeveer als volgt kunnen uitzien in Python: def model(x, tau = 0.99): weights_0 = array([[ 0.08104221, -0.03245814, ..., 0.04946151, 0.0736245 ]] weights_1 = array([ 0.1288752, 0.12100432, ..., 0.13083224, 0.02666049] y_dist = predict(x, weights_0, weights_1) y_hat_prob = max(y_dist) y_hat = y_dist.index(y_hat_prob) if(y_hat_prob &lt; tau): y_hat = None return y_hat In ons voorbeeld zou, indien x overeenkomt met \\(x_{300}\\) uit 2.5, y_dist overeenkomen met \\(P(\\hat{y}_{300}|x_{300})\\), y_hat_prob met \\(99.999893\\%\\) en y_hat met \\(6\\). Indien de berekende distributie 10 waarden zou bevatten die allen kleiner zijn dan \\(h_\\tau\\) (tau in de code hierboven), zou dit model aangeven dat er te weinig zekerheid is over welk cijfer de afbeelding moet voorstellen door de waarde None terug te geven. Stelling 2.5 Het is volstrekt normaal dat een ML model in sommige situaties geen voorspelling kan maken. 2.9 Het leerproces voor begeleid ML Zoals gezegd gaan we nog niet volledig in detail onderzoeken hoe een ML model tot stand komt, maar laten we toch al trachten het ML proces te schematisch reconstrueren, specifiek voor wanneer het gaat om begeleid leren. Wat weten we tot nu toe? We weten dat we vertrekken met training data \\(\\left(\\mathcal{X}_{trn},\\mathcal{Y}_{trn}\\right)\\) en eindigen met een model \\(\\hat{f}\\). We weten ook al dat het model een functie is die test data \\(\\mathcal{X}_{tst}\\) als invoer heeft en een voorspelling (eng: prediction) \\(\\hat{y}\\) als uitvoer (zie Figuur 2.8). Figuur 2.8: Eerste overzicht van het leerproces voor begeleid ML. Ronde vormen staan voor acties of functies, rechthoekige vormen geven objecten weer. Met andere woorden, het proces voor ML kan gecodeerd worden als \\(\\hat{f}=\\ell(\\mathcal{X}, \\mathcal{Y})\\). \\(trn\\) staat voor training, \\(tst\\) voor test. leren: eng. learning, training. voorspellen: eng. to predict, inference. 2.10 De onderdelen van een model Het model is dus een functie met daarin de gewichten \\(\\mathbf{\\theta}\\) alsook de nodige logica om de invoer op de juiste manier te bewerken met de gewichten. Dit laatste wordt in tekstboeken vaak over het hoofd gezien omdat het in de praktijk vaak neerkomt op een eenvoudige matrix vermenigvuldiging. Hier wordt ervoor gekozen om dit een beetje explicieter te maken. In het MNIST model uit vorige paragraaf zien we nog een derde element, het argument \\(h_\\tau\\) (tau in code). Dit is een hyperparameter. Hyperparameters die gekoppeld zijn aan een model dienen vaak, net als bij \\(h_\\tau\\), om een grenswaarde te bepalen tussen wat een goede voorspelling is en wat niet. We zullen in volgende paragraaf zien dat er ook hyperparameters gekoppeld kunnen worden aan het leeralgoritme, waar ze gewoonlijk een heel andere rol gaan spelen. Stelling 2.6 Het model dat het resultaat is van begeleid machinaal leren bestaat uit minstens 2 onderdelen: de gewichten \\(\\mathbf{\\theta}\\) de modellogica maar is vaak pas bruikbaar mits het definiëren van nog een derde onderdeel: de hyperparameters \\(\\mathbf{h}_f\\) 2.11 Hyperparameters Hyperparameters worden zo genoemd om ze duidelijk te onderscheiden van gewichten, die in de literatuur ook vaak parameters worden genoemd. Het onderscheid is belangrijk omdat hyperparameters het proces zelf beïnvloeden (e.g. waar moet je de lijn trekken, hoeveel getallen na de komma moet genereren, …) terwijl conventionele parameters het onderwerp zijn van de wiskundige bewerking waarmee invoer wordt omgezet naar een voorspelling. Stelling 2.7 Hyperparameters geven aan hoe een proces moet verlopen. Parameters dienen als invoer voor een functie. De manier waarop parameters tot stand komen worden bepaald door het leeralgoritme (zie volgende §). Hyperparameters, daarentegen worden door de datawetenschapper gekozen ofwel in een afzonderlijk proces automatisch geoptimaliseerd. De keuze voor de waarde van een hyperparameter zoals \\(h_\\tau\\) kan gemaakt worden door de datawetenschapper op basis van ervaring ofwel omdat het door de omgeving wordt opgelegd. Laten we twee voorbeelden bekijken die aangeven hoe de omgeving een drempelwaarde kan opleggen. Voorbeeld 2.1 Stel, er wordt de datawetenschapper gevraagd om een algoritme te ontwikkelen om afbeeldingen van katten en honden (en telkens exact één kat of één hond) te onderscheiden, gewoon voor het plezier. Stel dat het algoritme slechts één waarde uitvoert, namelijk de probabiliteit \\(p_{kat}\\) dat de afbeelding een kat laat zien. De probabiliteit dat het een hond is (\\(p_{niet kat}\\)), kan hieruit berekend worden met de formule \\(p_{niet kat}=1-p_{kat}\\). Een logische drempelwaarde op deze probabiliteiten is \\(0.5\\). Vanaf dat \\(p_{kat}\\) zakt onder deze waarde, gaan we ervan uit dat het om een hond gaat. Voorbeeld 2.2 De datawetenschapper wordt nu gevraagd om een algoritme te ontwikkelen om op basis van tal van bloedwaarden van een patiënt te bepalen of deze ziek is of niet. Het algoritme voert weer slechts één waarde uit, ditmaal de kans om ziek te zijn \\(p_{ziek}\\). De drempelwaarde mag nu niet meer zo maar op \\(0.5\\) worden geplaatst. Waarom niet? ML algoritmen maken fouten maar niet alle fouten zijn even erg. Het foutief aanduiden van een gezonde persoon als ‘ziek’ is niet zo erg als het foutief aanduiden van een zieke patiënt als gezond. Maar hoe weet de datawetenschapper dan welke drempelwaarde er gekozen moet worden? Nu komt het: in dit geval mag de datawetenschapper in geen geval een drempelwaarde zelf bepalen. De arsten die de opdracht geven dragen de verantwoordelijkheid om de ideale drempelwaarde te bepalen Stelling 2.8 Hyperparameters moeten vaak door andere mensen dan de datawetenschapper worden bepaald (ook als ze dat niet willen). 2.12 Het leeralgoritme Om van de training data naar het model te gaan heb je een leeralgoritme \\(\\ell\\) nodig. Voorbeelden hiervan zijn support vector machines (SVM), neuraal netwerken (NN), naïef Bayes algoritmen (NB), enzovoort… Er zijn wel honderden algoritmen waaruit een datawetenschapper kan kiezen, en voor elk algoritme zijn er gewoonlijk oneindig veel mogelijke instellingen1 (lett.). We begrijpen uit vorige paragraaf dat parameters en hyperparameters verschillend zijn maar puur wiskundig beschouwd, dienen ze allebei gewoon als invoer voor de ene of ander functie. Noteren we de hyperparameters bij het leeralgoritme nu als \\(\\mathbf{h}_\\ell\\), dan krijgen we het volgende: \\[\\hat{f}=\\ell\\left(\\{\\mathcal{X}, \\mathcal{Y}\\}, \\mathbf{h}_\\ell\\right)\\] Hier staat: het ML model wordt, in het geval van begeleid ML, gevormd door een leeralgoritme dat zowel data als invoer heeft (\\(\\{\\mathcal{X}, \\mathcal{Y}\\}\\)) als hyperparameters die het leerproces kunnen beïnvloeden. Wat is dan de rol van de hyperparameters die op het leeralgoritme inwerken? Zonder nu al te veel in gaan op het intern mechanisme van de leeralgoritmen, is één van de veel voorkomende functies van \\(\\mathbf{h}_\\ell\\) om te bepalen hoe snel het leeralgoritme te werk moet gaan. Ik wil hier nog even benadrukken dat het resultaat van het leeralgoritme moet bestaan uit een set van parameters én een functie-logica. Deze functie-logica ligt voor een bepaald leeralgoritme vast. Bijvoorbeeld, voor een logistische regressie, bestaat deze logica uit een logit-transformatie en een matrix vermenigvuldiging. Deze logica plus de parameters \\(\\mathbf{\\theta}\\) vormen samen het model. De hyperparameters die op het model inwerken worden, zoals uitgelegd in vorige §, meestal achteraf bepaald door de datawetenschapper of opgelegd door de omgeving. 2.13 Model complexiteit Merk op dat een ML model niet zo complex hoeft te zijn als dit MNIST model. Het kan best dat een model bestaat uit slecht een handvol parameters. Laten we een voorbeeld in beschouwing nemen. Voorbeeld 2.3 Een zeer bekende oefen-dataset is de iris dataset waarin afmetingen van de kelkbladen en kroonbladen van drie soorten irissen werden opgenomen. Bron: Szczecinkowaty (2007), Mayfield (2005), Mayfield (2007). Stel dat het doel is om de drie sterk gelijkende bloemensoorten te onderscheiden op basis van slechts 2 variabelen, namelijk de lengte van het kroonblad \\(x_p\\) en de lengte van het kelkblad \\(x_s\\). Veronderstel verder dat het model reeds getraind werd. Het model zou bijvoorbeeld kunnen afhangen van slechts 3 parameters die op \\(x_s\\) en \\(x_p\\) inwerken, bijvoorbeeld: \\(\\theta_1 = 2.5\\), \\(\\theta_2 = 1.2\\) en \\(\\theta_3=0.3\\). De logica van het model zou er dan bijvoorbeeld als volgt kunnen uitkzien: \\[species=\\begin{cases}\\text{setosa als }x_p&lt;\\theta_1\\\\\\text{versicolor als }x_s&gt;\\theta_2p+\\theta_3\\\\\\text{virginica anders}\\end{cases}\\] In dit geval kan de model-logica als volgt gecodeerd worden (Python): def model(x): if x.p &lt; 2.5: return &quot;setosa&quot; if x.s &gt; 1.2 * x.p + .3: return &quot;versicolor&quot; return &quot;virginica&quot; Deze versie van het iris-model met slechts 3 gewichten (parameters!) is wel erg eenvoudig maar er staan ML modellen in productie die niet veel complexer zijn en toch hun werk doen. Complexiteit is trouwens geen goede maatstaf voor de ‘kwaliteit’ van een model. Soms eisen de vragende partij of andere belanghebbenden in het AI project nadrukkelijk een zo eenvoudig mogelijk model zodat het begrijpbaar en transparant blijft. Aan de andere kant bestaan er ook modellen die nog véél complexer zijn dan het MNIST model met honderden miljarden gewichten (\\(J &gt; 10^{11}\\); Shazeer et al. (2017))! Stelling 2.9 Model complexiteit is geen goede maatstaf voor de ‘kwaliteit’ van een model. 2.14 Comprimeren door middel van een ML model Uit de vorige paragraaf leren we dat modellen zeer eenvoudig of zeer complex kunnen zijn. Meestal bevat het model inclusief de gewichten weliswaar véél minder informatie dan de oorspronkelijk data waarop getraind werd. In deze gevallen kan je het model beschouwen als een vereenvoudigde voorstelling van de werkelijkheid of nog als een geavanceerde ‘zip-functie’, hetzij eentje die niet volmaakt is (eng: lossless). Het zou ons te ver leiden om de compressie-factor (eng: compression ratio) te berekenen op basis van de Informatietheorie, maar misschien kunnen we een grove schatting maken van de compressie-factor voor ons MNIST-model (zie Voorbeeld 2.4). Voorbeeld 2.4 De training set bestaat uit 60 000 afbeeldingen van 28 × 28 pixels. Met de grijswaarden in enkelprecisievariant (eng: single precision) van de zwevendekomma getallen komt de grootte van de training set op: \\[60\\,000\\times28\\times28\\times32\\approx1.5\\cdot10^9 bits\\] Daartegenover bestond het MNIST-model uit \\(100\\,480\\) gewichten: \\[100\\,480\\times32\\approx3.2\\cdot10^6 bits\\] Dit brengt ons op een compressie-factor van: \\[\\frac{1.5\\cdot10^9 bits}{3.2\\cdot10^6 bits} &gt; 450\\] Oefening 2.2 (Iris compressie) Probeer nu zelf ook eens de compressiefactor in te schatten voor het iris-model uit Voorbeeld 2.3. Zei opties meerkeuzevraag op Digitap. 2.15 Leren versus ontwerp We hebben al heel wat woorden vuil gemaakt om te begrijpen wat ML eigenlijk inhoud. Laten we nu even stilstaan bij wat ML niet is. Om te beginnen is ML niet ontwerpen. Wat is het verschil? Ook hier is het gemakkelijkste om het verschil aan te tonen aan de hand van twee voorbeelden. Voorbeeld 2.5 (Ontwerp-benadering verkoopsautomaat probleem) De probleemstelling is als volgt: Maak een verkoopautomaat die op basis van de massa en de diameter de 4 verschillende Indiase muntstukken (0.5, 1, 2 en 5 Rs) zo goed mogelijk kan onderscheiden. Oplossing via ontwerp: Zoek naar de specificaties die gebruikt worden tijdens het ontwerpen van de munstukken. Op de webpagina van Takashi Shimazaki vind je een samenvatting: Waarde (Rs) Massa (\\(g\\)) Diameter (\\(mm\\)) 0.5 3.79 22.0 1 4.85 25.0 2 5.62 27.0 5 6.00 23.0 We veronderstellen verder een standaardafwijking op de metingen van \\(0.09g\\) en \\(0.1 mm\\) en een fouttolerantie van maximaal 0.1%. Op basis van deze gegevens stellen we een grafiek op en beslissen we ‘met de hand’ waar de grenswaarden worden gedefiniëerd: Voorbeeld 2.6 (ML-benadering van het verkoopsautomaat probleem) Oplossing via ML: zoek naar data met werkelijke metingen van de massa’s en diameters van de verscheidene Indiase munstukken. Bij gebrek aan zulke data zit er niets anders op dan een random steekproef te nemen van de munstukken en ze zelf te meten (of door iemand betrouwbaar te laten meten). Hier is het resultaat van de metingen: verkoopautomaat-ml De kleuren stellen het resultaat voor van een clusteralgoritme (k-means), dit is trouwens een type algoritme dat onbegeleid tewerk gaat en verschilt dus fundamenteel van het begeleid ML dat we tot hiertoe bespraken. Uit deze data lijkt het of de spreiding veel groter is dan aangenomen in vorig Voorbeeld. Daardoor maakt het model hier vermodelijk een aantal fouten in de classificatie. Het model ‘weet’ hier ook niet welke cluster overeenkomt met bijvoorbeeld 2 roepie. De enige input, naast de diameters en de massa’s van de muntstukken is dat er 4 clusters in het totaal moeten zijn. De data in dit voorbeeld werden gesimuleerd, zie hieronder voor meer detail. De grafiek voor ontwerp benadering kan je genereren met de onderstaande R code: library(plotrix) coins &lt;- fread(&quot; Waarde\\tMassa\\tDiameter 0.5\\t3.79\\t22.0 1\\t4.85\\t25.0 2\\t5.62\\t27.0 5\\t6.00\\t23.0&quot;) plot(0, type = &quot;n&quot;, xlim = c(21, 28), ylim = c(3, 7), xlab = &quot;Diameter&quot;, ylab = &quot;Massa&quot;) alpha &lt;- .001 dummy &lt;- coins[, draw.ellipse(Diameter, Massa, qnorm(1 - alpha, 0, 0.1), qnorm(1 - alpha, 0, 0.09))] segments(c(25, 22.5, 25.2), c(3, 4.7, 6), c(21, 26.5, 28), c(5.5, 7, 3), lty = 3) dummy &lt;- coins[, text(Diameter, Massa, Waarde)] Hieronder vind je de code terug voor de ML-benadering uit Voorbeeld 2.6. Merk op dat ik hier, omdat er niet direct data voorhanden was, de data zelf heb gesimuleerd. library(MASS) set.seed(42) # Generating data copula &lt;- function(lab, n, rho, mu1, mu2, sd1, sd2){ mu &lt;- c(mu1,mu2) sigma &lt;- matrix(c(sd1 ^ 2, sd1 * sd2 * rho, sd1 * sd2 * rho, sd2 ^ 2),2) mvrnorm(n, mu, sigma) %&gt;% as.data.table %&gt;% set_names(c(&quot;Diameter&quot;, &quot;Mass&quot;)) %&gt;% cbind(Label = lab) } coins_meas &lt;- coins[1, copula(&quot;0.5&quot;, 500, .3, Diameter, Massa, .8, .5)] %&gt;% rbind(coins[2, copula(&quot;0.5&quot;, 500, .3, Diameter, Massa, .8, .5)]) %&gt;% rbind(coins[3, copula(&quot;0.5&quot;, 500, .3, Diameter, Massa, .8, .5)]) %&gt;% rbind(coins[4, copula(&quot;0.5&quot;, 500, .3, Diameter, Massa, .8, .5)]) # Analyzing data model &lt;- kmeans(coins_meas, centers = 4, iter.max = 1000) plot(coins_meas$Diameter, coins_meas$Mass, xlab = &quot;Diameter&quot;, ylab = &quot;Massa&quot;, pch = 19, col = model$cluster, cex = .6) text(model$centers[,&quot;Diameter&quot;], model$centers[,&quot;Mass&quot;], LETTERS[1:4]) 2.16 Leren versus onthouden en inferentie De moeilijkste te onderscheiden begrippen in de wereld van ML zijn leren versus inferentie. Dit probleem komt trouwens deels overeen met het onderscheid tussen respectievelijk ML en regressie. Zich de moeite getroosten om een model te ontwikkelen heeft alleen maar nut indien het ook achteraf gebruikt zal worden om voorspellingen mee te doen. Met andere woorden om dingen te ontdekken die men (t.t.z. de computer) nog niet wist. Laten we onderstaand voorbeeld onderzoeken om te begrijpen wat het precies betekent om niets nieuw te leren. Voorbeeld 2.7 (MT cars relatie) De Motor Trend Car Road Tests dataset bevat 10 standaard eigenschappen voor de motoren van 32 automerken zoals verbruik, cilinderinhoud en dergelijke. Deze data zijn net als iris data een waarde geworden in de wereld van de statistiek. De data werd oorspronkelijk verzameld uit het Motor Trend US magazine van 1974 (zie Henderson and Velleman (1981)). Geef ?mtcars in R om meer te weten te komen. Stel nu dat er een theorie bestaat die de cilinderinhoud \\(c\\) in verband brengt met de massa \\(w\\) van de wagen door middel van een variant van de zogenaamde hill-functie: \\[c=\\theta_1+\\frac{\\theta_2-(w-\\theta_3)^2}{\\theta_4^2+(w-\\theta_3)^2}\\] De onderzoeker zal trachten deze vraag te beantwoorden. Hij voert een niet-lineare regressie analyse uit. Hierbij zal een zogenaamd optimalisatie-algoritme de 4 parameters wijzigen totdat de functie die zo ontstaat zo goed mogelijk overeenkomt met de aangeleverde dataset. De onderstaande clip brengt dit proces tot leven: cars-regressie Naarmate dat de optimalisatie vordert (zie Nelder (1965)), verkleint de afwijking tussen voorspelde curve met de datapunten, zoals te zien is aan de zogenaamde Root mean squared Error (RMSE). Op het einde van de optimlisatie, bij een RMSE van 52.527, komt het algoritme tot stilstand met de volgende parameter-waarden: \\[\\theta_1=78, \\theta_2=517.47, \\theta_3=1.87, \\theta_4=2.10\\] Wat het Voorbeeld 2.7 laat zien is gebaseerd op een vooraf bepaalde model-logica. Van dit model mogen wel de parameters maar niet de complexiteit noch de formule gewijzigd worden. Er kan inderdaad een ideale parameter-set gevonden kan worden die bij deze 32 datapunten past. Maar wat als dezelfde opdrachtgever nu tegen de onderzoeker zegt dat die zich vergist had. Het zou beter zijn om te de puntenwolk te beschrijven door middel van een polynoom van de vijfde graad. De onderzoeker zet zich eraan en schrijft de volgende R code: rmse &lt;- function(y, x){ sqrt(mean((y - x) ^ 2)) } model &lt;- lm(disp ~ poly(wt, 5), data = mtcars) rmse(model$fitted.values, mtcars$disp) Het resultaat is nu een RMSE van 52.492, beter dan het vorige resultaat, alleen ziet de resulterende curve er nu een beetje vreemder uit: Het probleem is natuurlijk dat hoe hoger de graad van de polynoom, hoe beter de fit zal zijn (is een wiskundige zekerheid) maar ook hoe ‘lelijker’ de curve zal worden: Wat hier aan het gebeuren is noemt met overfit. Een polynoom met zulke hoge graad is veel te complex voor het onderliggend patroon. We voelen dit ergens wel aan. Voordat ML populair was, zou een onderzoeker ‘op het gevoel’ afgaan om de graad van de polynoom te bepalen. In ML wordt de complexiteit niet op voorhand opgegeven en zoekt het leeralgoritme zelf uit hoe complex het model mag worden alvorens er overfit optreedt. Kunnen we dat overfitten ook formaliseren of zelfs bewijzen? Het antwoord is ja en ja. Het eerste (het formaliseren) zou ons te ver leiden, maar voor de geïnteresseerden raad ik aan om de term VC dimension op te zoeken, genoemd naar twee grondleggers van de ML theorie: Vladimir Vapnik en Alexey Chervonenkis en het boek (of online lezingen) van Yaser Abu-Mostafa (zie (thm: abumostafa)) te bekijken (Abu-Mostafa et al. (2012)). Het tweede aspect (het bewijzen) is eenvoudiger. We moeten gewoon op zoek gaan naar nieuwe data: We zien nu het gevolg van overfitting. Wanneer we het model blootstellen aan nieuwe data zien we dat het eenvoudige model (polynoom van graad 5) veel beter presteert dan het complexere model (polynoom van graad 7). En dat is het hem nu net allemaal om te doen! Leren om voorspellingen te maken, niet leren om de training dataset nauwkeurig te beschrijven, want dat is gewoon onthouden of hoogstens comprimeren zoals we eerder zagen. Daarom is ML veel interessanter dan regressieanalyse. Je kan stellen dat regressie (zoals de naam het eigenlijk al aangeeft) gaat over ‘achterom kijken’, terwijl ML gaat over ‘naar de toekomst’ kijken. 2.17 Onbegeleid ML Tot hier toe werd er voornamelijk gesproken rond begeleid ML (eng: supervised machine learning). Er zijn nog minstens twee andere vormen die binnen sommige disciplines erg populair zijn. In Voorbeeld 2.6 werd wel gesproken van clusters, en dat is een voorbeeld van onbegeleid ML (eng: unsupervised machine learning of self-organization). Het verschil tussen begeleid en onbegeleid is het gebruik van de uitkomsten (\\(y\\)) als invoer voor het leeralgoritme. Stelling 2.10 Bij begeleid ML wordt het leeralgoritme getraind op invoer data (\\(\\mathbf{x}\\); de onafhankelijke variabelen) die paarsgewijs gekoppeld zijn met uitvoer data (\\(\\mathbf{y}\\); de afhankelijke variabele of uitkomst). Het algoritme leert de verbanden tussen \\(\\mathbf{x}\\) en \\(\\mathbf{y}\\). Bij onbegeleid ML is er geen sprake van een uitkomst en leert het algoritme gewoon patronen te herkennen in de invoer data. Figuur 2.9: Vergelijking tussen het proces van begeleid L en het proces van onbegeleid ‘leren’. Welke van beide moet je nu gebruiken? Heel simpel: als je een betrouwbare uitkomst in je bezit hebt, dan kies je best begeleid ML, omdat deze altijd beter zal presteren. Heb je geen uitkomsten of twijfel je aan de authenticiteit of accuraatheid, dan kan je (tijdelijk) overstappen op onbegeleid leren. Binnen onbegeleid leren, zijn er twee families die veel gebruikt worden. Eentje is de principale componenten analyse (PCA; eng: principal component analysis), de ander is cluster analyse (eng: cluster analysis). In beide gevallen is het resultaat een categorisatie van de instanties (zoals bijvoorbeeld een clustering) of een set van associaties (zoals bijvoorbeeld aanbevelingen, eng: customer recommendations). Stelling 2.11 Een onbegeleid ML algoritme vertelt je of twee instanties bij elkaar horen (grote kans op associatie, zelfde cluster, …) of niet. Laten we de discussie van onbegeleid ML afsluiten met een voorbeeld: Voorbeeld 2.8 Stel, een klant wil een geautomatiseerd systeem ontwikkelen om de scherpte meten van microtoommessen. De klant hoopt dit te kunnen doen a.d.h.v. microscopische opnamen van de snede van het mes, waarvan hieronder een voorbeeld: Het doel is om de oppervlakte te bepalen van de fellere horizontale lijn (de messnede), maar om dit te kunnen doen moet er een duidelijk onderscheid worden gemaakt tussen fellere pixels en donkere pixels. M.a.w., elke pixel moet gecategoriseerd worden als ‘messnede’ of ‘achtergrond’. Een begeleid ML aanpak zou inhouden dat een mens heel wat afbeeldingen manueel beoordeeld en dat er een classificatie algoritme gevoed zou worden met deze uitkomst data. Alleen is dat hier niet erg praktisch en bovendien erg onbetrouwbaar. Vandaar de keuze voor onbegeleid leren. Elke pixel in bovenstaande afbeelding bevat een R, een G en een B waarde. Het doel is om de afbeelding om te zetten naar een grijswaarde afbeelding, maar dan wel met de optimale gewichten voor de afzonderlijke R, G en B kleur-kanalen zodat er een maximaal contrast ontstaat. We zien immers dat de snede van het mes groen-achtig en niet wit is en we kunnen dus vermoeden dat de drie kleuren een verschillend gewicht gaan krijgen bij het zoeken naar het hoogste mogelijk contrast. Hieronder wordt de code getoond (imports en een aantal helperfuncties weggelaten; zie broncode voor details): predict_pca &lt;- function(x, n) { x$x[, 1:n] %*% t(x$rotation[, 1:n]) %&gt;% scale(scale = FALSE, center = -1 * x$center) %&gt;% as.data.table } contrast &lt;- function(img) { img %&gt;% img_to_dt %&gt;% prcomp( ~ R + G + B, data = .) %&gt;% predict_pca(1) %&gt;% dt_to_img(dim(img)) %&gt;% grayscale %&gt;% normalize } par(mar = rep(0, 4)) img &lt;- &quot;img/Microtome_knife_5.0_5.0_0097.JPG&quot; %&gt;% load.image %&gt;% contrast %&gt;% plot(axes = FALSE) De code leest als volgt: Laad de afbeelding Microtome_knife_5.0_5.0_0097.JPG en in de contrast functie: zet je het beeld eerst om naar een data.table met de helper-functie img_to_dt (niet getoond). Start daarna de principale componenten analyse op met de ingebakken stats::prcomp functie. Geef hierbij mee dat de drie kleur-kanalen moeten gebruikt wordt om het grootste contrast te vinden. Neem het resultaat van de analyse en neem de eerste component (de combinatie van R, G, en B-waarden die het meeste variantie verklaart) en voer daarmee de omgekeerde bewerking uit met de predict-pca functie. Het resultaat is dat er een iets beter drempel-waarde (eng: threshold) gevonden kan woorden waarme de messnede geïdentificeerd kan worden: Demo PCA results Het probleem met onbegeleid ML is dat je nooit zeker weet dat je echt iets geleerd hebt. Bijvoorbeeld, indien een onbegeleid model een nieuwe instantie onderbrengt in een welbepaalde cluster, hoe weet je dan of het juist is of fout? Daarom is die term “onbegeleid leren” mogelijk nogal verwarrend en is het vaak veiliger om gewoon te spreken van clustering (bijv. k-means) of eigen-decompositie (bijv PCA.). 2.18 Conditionering Er is nog een derde soort ML, namelijk de conditionering (eng: reinforcement learning). In plaats van een afhankelijke variabele, wordt de uitkomst aangeleverd als een functie die, gegeven een bepaalde input, de uitkomst zal teruggeven. Het grootste verschil met de twee eerder besproken vormen van ML is dat er voor conditionering geen nood is aan historische data. Stelling 2.12 Voor conditionering (reinforcement learning) heb je geen historische data nodig. Enkel een tolk die in staat is een actie van een agent te beoordelen. Conditionering kent toepassingen in erg veel verschillende disciplines waaronder de statistiek, maar ook de speltheorie, meet- en regeltechniek, operationeel onderzoek, de informatietheorie, enzovoort…. Het werkt zo. Een software-agent (eng: agent) krijgt de keuze uit een aantal acties (eng: actions) die het kan uitvoeren op een omgeving (eng: environment). Het (voorlopig immatuur) model (een tolk; eng: interpreter) vertaalt de gekozen actie naar enerzijds een toestand-wijziging (eng: state change) en anderzijds een beloning (eng: reward). Figuur 2.10: Overzicht proces voor conditionering. Een toestand ruimte wordt gedefinieerd of er wordt een functie voorzien die kan meegeven of een toestand mogelijk is of niet. Daarnaast moet er een beloning systeem worden meegegeven zodat de tolk voor elke mogelijke handeling een beloning kan bepalen (of bestraffing door middel van een negatieve beloning). We komen later meer uitgebreid terug op conditionering, maar nu houden we het bij een minimalistische implementatie van een soort conditionering, genaamd Q-learning, dit maal in Python. Het voorbeeld is afkomstig van de blog “The Beginner Programmer” van Mic en hetgeen op zijn beurt gebaseerd is op een Q-learning handleiding op de Mnemosyne_Studio blog van John McCullock. Voorbeeld 2.9 Gegeven onderstaande plattegrond van een huis met vijf kamers, vind het korst mogelijke pad naar buiten als je je in kamer 2 bevindt. Figuur 2.11: De plattegrond. (bron) De belonging kan voorgesteld worden als een matrix: Figuur 2.12: De beloning-matrix ( bron) import numpy as np reward = np.matrix( [[-1,-1,-1,-1, 0, -1], [ -1,-1,-1, 0,-1,100], [ -1,-1,-1, 0,-1, -1], [ -1, 0, 0,-1, 0, -1], [ -1, 0, 0,-1,-1,100], [ -1, 0,-1,-1, 0,100]]) We maken nu een matrix die het geheugen voorstelt van de tolk. Hierin zal de tolk zijn eerdere ervaringen in opslaan. Dit geheugen stelt de toestand (eng: state) voor waarin de agent zich bevindt. Zoals je ziet weet de agent initieel helemaal niets. memory = np.matrix(np.zeros([6, 6])) In andere situaties kan het aantal toestanden waarin een agent zich kan bevinden helemaal niet gedefinieerd zijn en dan moet de matrix dynamisch worden opgesteld. Het opvullen van de toestand-matrix gebeurt volgens de onderstaande transitie regel (eng: transition rule): \\[ Q(toestand, actie)=R(toestand, actie)+h_\\gamma\\cdot max(Q(volgende\\,toestand,\\,alle\\,acties)) \\] We hebben nu een functie die, gegeven een bepaalde toestand, de set van mogelijke volgende toestanden teruggeeft (available_actions) en een tweede die gegeven de mogelijke toestanden er een willekeurige toestand uitkiest (sample_next_action). def available_actions(state): current_state_row = reward[state,] av_act = np.where(current_state_row &gt;= 0)[1] return av_act def sample_next_action(available_actions_range): next_action = int(np.random.choice(available_act,1)) return next_action Ten slotte hebben we een update-functie, de instap-functie a.h.w. van het leeralgoritme, die het geheugen (de toestand-matrix) vernieuwd: def update(current_state, action, gamma): max_index = np.where(memory[action,] == np.max(memory[action,]))[1] if max_index.shape[0] &gt; 1: max_index = int(np.random.choice(max_index, size = 1)) else: max_index = int(max_index) max_value = memory[action, max_index] memory[current_state, action] = reward[current_state, action] +\\ gamma * max_value We initialiseren nu het spel met de leersnelheid \\(h_\\gamma\\) en de initiële kamer (2) en de doel-ruimte (buiten, ruimte 5) en voeren de eerste update uit: gamma = 0.8 initial_state = 2 end_state = 5 Nu begint het trainen (10 000 iteraties): for i in range(10000): current_state = np.random.randint(0, int(memory.shape[0])) available_act = available_actions(current_state) action = sample_next_action(available_act) update(current_state, action, gamma) De toestand-matrix er nu zo uit: model &lt;- as.matrix(100 * py$memory / max(py$memory)) dimnames(model) &lt;- list(start = 0:5, stop = 0:5) model ## stop ## start 0 1 2 3 4 5 ## 0 0 0 0.0 0 80 0 ## 1 0 0 0.0 64 0 100 ## 2 0 0 0.0 64 0 0 ## 3 0 80 51.2 0 80 0 ## 4 0 80 51.2 0 0 100 ## 5 0 80 0.0 0 80 100 Nu testen we het model met een begintoestand van 2 en een eindtoestand van 5: current_state = initial_state steps = [current_state] while current_state != end_state: next_step_index = np.where( \\ memory[current_state,] == np.max(memory[current_state,]))[1] if next_step_index.shape[0] &gt; 1: next_step_index = int(np.random.choice(next_step_index, size = 1)) else: next_step_index = int(next_step_index) steps.append(next_step_index) current_state = next_step_index Het aangeleerd pad is als volgt: py$steps %&gt;% paste(&quot;Ruimte &quot;, ., collapse = &quot; &gt; &quot;) ## [1] &quot;Ruimte 2 &gt; Ruimte 3 &gt; Ruimte 1 &gt; Ruimte 5&quot; Bronvermelding "],
["data.html", "Hoofdstuk 3 Data 3.1 Data voor ML 3.2 Wat is data 3.3 Soorten data 3.4 Externe databronnen 3.5 Data Genereren 3.6 De analyse dataset 3.7 Soorten variabelen 3.8 (Eng) Nominal-Scale Data 3.9 (Eng) Dummy Variables 3.10 (Eng) Ordinal-Scale Data 3.11 (Eng) Circular-Scale 3.12 (Eng) Censoring 3.13 Tijd en ruimte 3.14 Toegang tot data 3.15 Het codeboek", " Hoofdstuk 3 Data 3.1 Data voor ML Leeralgoritmen gebruiken data om er een model uit te destilleren. Als er iets fout gaat, dan heeft dit vaak te maken met de staat waarin de data zich bevinden of de manier waarop er met de data wordt omgesprongen. Stelling 3.1 When it comes to ML, garbage in, garbage out (GIGO). De datawetenschapper streeft ernaar om de data die hij of zij te zien krijgt zo goed mogelijk te begrijpen. Wiskundig gezien is dat misschien niet zo belangrijk, maar het gaat hier over enerzijds het maken van de juiste keuzes en anderzijds voor de ethische overwegingen. Als je van plan bent om uitspraken te doen op basis van data, ben je ook verantwoordelijk voor het opbouwen van een grondig begrip van wat de data voor staat. Figuur 3.1: “The gossip” (ned. De roddel) van Norman Rockwell. Opmerking Als datawetenschapper heb je, indien de data betrouwbaar is en je voldoende integer bent om je van alle vooroordelen en taboes te ontdoen, toegang tot de ultieme waarheid. Ben je er klaar voor? 3.2 Wat is data Bij dit principe moeten we tegenwoordig niet erg lang stilstaan. Het is ondertussen voor de meeste mensen duidelijk dat overal waar je kijkt er data zijn. Alles wat je online doet maar ook alles wat je offline doet kan tot data worden omgevormd. Om als invoer te kunnen dienen voor een leeralgoritme, moeten de data in de eerste plaats natuurlijk beschikbaar zijn. En daar wringt het eerste schoentje al. Want ‘beschikbaar’ betekent niet alleen dat je er toegang tot hebt, maar ook dat de data zich in het juiste formaat bevinden om te kunnen gebruiken door het leeralgoritme. 3.3 Soorten data Ook hier moeten we niet te lang bij blijven stilstaan. We bekijken wel een aantal vaak gebruikte termen. Interne data: Private data die behoren tot een organisatie. Externe data: Publieke data of private data van een andere organisatie Ongestructureerde data: Data die niet in een platte tabel-structuur kan worden weergegeven. Als het gekozen leeralgoritme hier niet mee overweg kan, moet je eerst features extraheren en zelf een platte tabel maken (eng: melting data) Metadata: Is data over data zoals tijdstip van opname, gebeurtenissen uit logboek, enz… 3.4 Externe databronnen Veel gegevens worden intern bij organisaties bewaard in afzonderlijke, speciaal daarvoor voorziene databanken, bestanden of (zeker als het om historische gegevens gaat) de fysische tegenhangers zoals rekenbladen, werkboeken, documenten, archieven, …. Vaak zijn er echter nog andere bronnen zoals audit-rapporten, usage-statistieken rond het gebruik van websites, logbestanden, videobeelden, … waar men niet onmiddellijk aan denkt. Binnen een bedrijfs-context is het net belangrijk om nieuwe databronnen aan te boren, bronnen waar de concurrentie niet eerder aan dacht. Hieronder een collage van een aantal potentiële interessante bronnen van data op het internet (naast de meest vanzelfsprekende zoals sociale media e.d.). Het gaat hier om generieke bronnen met data over meerdere domeinen. Academictorrents Amazon Web Services public datasets Brits open data platform Canadese open data site Common Crawl DataQuest Data World Europese Open data portaal FiveThirtyEight Forbes Gapminder Google Trends, Scholar, Patents, Google Dataset Search lab, … Kaggle Reddit Socrata Standaard datasets UCI Machine Learning Repository US Census Bureau US Open data portal Quandl Quantopian Wunderground enzovoort… Hierboven worden een aantal databronnen opgesomd, maar het vergt vaak wel wat meer speurwerk om de ideal dataset te vinden. We gaan hier even een fictief voorbeeld aanhalen van hoe zo een zoektocht zou kunnen verlopen. Voorbeeld 3.1 Op zoek naar data (hypothetisch): je bent op zoek naar een gratis beschikbare data die een voorspellende waarde heeft voor het transportsnelheid van goederen over de Westerschelde. Je denkt dat windsnelheid misschien een impact kan hebben op de vaarsnelheid. Dit zou je zoektocht naar geschikte data bijvoorbeeld kunnen zijn: Je zoekt eerst naar een website die historische windsterkte vrijgeeft, liefst via API Je vind wel een aantal bronnen maar geen daarvan zijn én gratis én in staat om historische data op te halen Dan lees je ergens dat golfslag misschien een betere voorspellende waarde heeft Je zoekt op Google naar Wave Measurement en je komt dit wetenschappelijk artikel tegen In dat artikel blijken ze exact te doen wat je naar op zoek bent Daarom zoek je in dat artikel naar de ‘Methods’ sectie om te achterhalen waar de auteurs hun data vandaan hebben (een wetenschappelijk artikel moet per definitie reproduceerbaar zijn dus moet het zijn bronnen altijd vermelden) Je vind dat ze de data van de organisatie NOAA hebben en je geeft het in op Google Op de website van de NOAA ontdek je dat ze de gegevens publiek ter beschikking stellen Er zijn data voor België beschikbaar, maar de frequentie waarmee ze beschikbaar worden gemaakt is toch teleurstellend Je realiseert je plots dat er misschien Belgische sites zijn die ook hun gegevens delen Je komt op de website van het Meetnet Vlaamse Banken en daar vind je, na inloggen, exact de data die je nodig hebt… 3.5 Data Genereren Natuurlijk is er de optie om zelf data te genereren. In principe is dat iets dat typisch in het domein van de wetenschap valt omdat hierbij strikte deontologische regels te volgen zijn. De regels hebben er eigenlijk allemaal mee te maken dat de waarnemer zeker is (en dat ook kan aantonen) dat de waarnemingen betrouwbaar, accuraat, precies, ethisch verantwoord en relevant zijn. Dit is een niet te onderschatten taak en er is heel wat ervaring nodig om dit te kunnen bereiken. De correcte manier om dat te doen valt buiten het bereik van deze cursus maar geïnteresseerde lezers kunnen hiermee beginnen: Carey, S. S. (2011). A beginner’s guide to scientific method. Cengage Learning. 3.6 De analyse dataset De data kan allerlei vormen aannemen maar de leeralgoritme kunnen niet zomaar overweg met al die verschillende vormen. Vaak wil de datawetenschapper de data eerst integreren tot een enkel bestand of data-stroom met een ‘platte’ tabel-structuur, dit noem men de analyse dataset. Stelling 3.2 Een leeralgoritme vereist meestal een analyse-dataset met een ‘platte’ structuur. Dit betekent één tabel met één instantie per rij en één variabele per kolom. Figuur 3.2: De onderdelen van een dataset met ‘platte’ structuur. 3.7 Soorten variabelen Het gaat hier over de data type van de variabele, bijvoorbeeld of het gaat over een getal of over tekst. Dit lijkt sterk op de verschillende data types die men in een programmeertalen tegenkomt (integer, string, …). Maar die overeenkomst is maar oppervlakkig. Binnen de statistiek is men verplicht om meer data types te onderscheiden die niet hun gelijke kennen binnen de IT. Laten we beginnen met de meest voorkomende data types in een grafiek te zetten: Figuur 3.3: Schematische visualisaties voor een aantal veel voorkomende data types Let op de labels op elke grafieken. ze geven telkens een realistisch scenario weer. Je kan deze enkelvoudige data types indelen volgens drie dimensies: Numeriek vs ordinaal vs nominaal: Met numerieke data kan je berekeningen maken, met ordinale gegevens kan je enkel ongelijkheid testen en met nominale gegevens kan je noch berekenen noch ongelijkheid testen Continu vs discreet: Bij een continue variabele kan je tussen twee willekeurig gekozen waarden oneindig veel andere waarden plaatsen. Bij een discrete variabele kan dat niet. Continue variabelen worden verder onderscheiden van op basis van de meetschaal. Bij een intervalschaal is er geen betekenisvol nulpunt (zoals bij temperatuur in graden Celcius) bij een getal met ratioschaal is dat wél het geval. Circulair vs lineair: Bij circulaire data is de as op zichzelf teruggeplooid om een cirkel te vormen. 3.8 (Eng) Nominal-Scale Data Whenever the data consist of categories, typically, we deal with so-called nominal-scale data. Examples are department, blood type, country, …. Whenever there are only two categories, we speak of binary data (true/false, yes/no, success/failure, ….). 3.9 (Eng) Dummy Variables Whenever we encounter a categorical feature, we may have to consider to create a set of dummy variables out of a single categorical variable so as to support statistical analysis in general and ML using a specific learner in particular: Figuur 3.4: Demonstration of the use of dummy variables. 3.10 (Eng) Ordinal-Scale Data In some cases, the categories in categorical data have some natural ordering such as is the case for ranking or scoring measures (e.g. “good”, “medium”, “bad”). Coding such a variable into a discrete numerical one is not very good from a statistical point of view (because then you assume a ratio scale which may not be realistic) but sometimes understandable from the practical point of view, especially if, for your particular problem, finding an appropriate learner can be hard. There is also the possibility to treat the ordinal-scale variable as a categorical one (and e.g. creating a set of dummy variables) but be aware that you lose information along the way. The same is true for other forms of discretizations, by the way. Remark One can always convert a variable with a rich data type to one with poor data type. This can benefit practicality be always means losing information. 3.11 (Eng) Circular-Scale Whenever there is no maximum or minimum value for a scale, it is said to be circular-scaled. Examples of such measurements are time of day, day of week, wind direction, bearing, …. This type of scale can be super-imposed on other scale types and can be both discrete and continuous. For example, direction measured relative to e.g. the magnetic north is a interval-scaled continuous circular measure and the hour of the day follows a discrete circular scale. Mind that circular scale data is very hard to work with as even simple operations such as a mean requires special attention: Figuur 3.5: Circular scale data requires special attention. 3.12 (Eng) Censoring Sometimes data is according to a truncated scale. For example, in clinical data sets you frequently encounter situations where as patient is said to survive for at least x years after the treatment, indicated as &gt;x (e.g. &gt;12 years). For such a truncation we call the data right-censored. Similarly lab measurements are frequently reported to be below a certain limit without knowing the exact number such as &lt;1.03 mmol/L HDL, this is called a left-censored datum. In case we have censoring at both ends of the scale, we speak of interval censoring (e.g. 2 &lt; x &lt; 5 births per day). It remains important to try to understand the reason of the censoring. It could be that the measurement was not sufficiently accurate or maybe it was done accurately but afterwards categorized (placed into bins) during some preprocessing step. In case you need to deal with censoring there is a limited number of statistical tools to your disposal. One is the use of tobit regression. 3.13 Tijd en ruimte Data is een spiegel voor allerlei informatie uit de reële wereld. Toch zijn er twee grootheden die eruit steken. Het gaat om de twee dimensies tijd en ruimte. Andere variabelen zijn vaak secundair aan deze dimensies. Voorbeelden zijn het voorspellen van de toekomstige verkoopcijfers of de verspreiding van een virus tijdens een pandemie. Laten we naar een bekende (maar gedateerde) dataset kijken van de farmareus Johnson &amp; Johnson: plot (JohnsonJohnson, main = &quot;Kwartaalcijfers J&amp;J&quot;) Het gaat om de kwartaalcijfers van 1960 to 1980. Deze dataset wordt vaak in educatieve werken rond tijdsreeks-analyses gebruikt om het effect van een multiplicatieve seizoensafhankelijke trend (eng: seasonality) weer te geven. Multiplicatief betekent hier gewoon dat het seizoenseffect afhangt van de grootte van de waarde. We voelen meteen aan dat deze tijdsreeksen van een heel andere soort data zijn dan de eerder aangehaalde data. Dat heeft te maken met het feit dat een punt op dit lijndiagram niet onafhankelijk is van een ander punt op dezelfde lijn. We spreken van autocorrelatie. In het algemeen, wanneer men onderzoek doet naar de evolutie van gegevens in de loop van de tijd, spreekt van van trend analyse. Hetzelfde kunnen we nu doen voor ruimte. Onderstaande figuur laat zien hoe we bijvoorbeeld aantallen kunnen uitdrukken over een ruimtelijke dimensie (met toestemming van Hermans (2019)). # From https://github.com/mhermans/thematic-maps-r # Thanks to Maarten Hermans library(BelgiumMaps.StatBel) library(sf) library(tmap) data(BE_ADMIN_MUNTY) munip_map &lt;- st_as_sf(BE_ADMIN_MUNTY) src &lt;- paste0( &quot;https://raw.githubusercontent.com/mhermans/&quot;, &quot;thematic-maps-r/master/data/muni_typology.csv&quot;) munip_data &lt;- read.csv(src) munip &lt;- merge(munip_map, munip_data, by.x = &quot;CD_MUNTY_REFNIS&quot;, by.y = &quot;gemeente_nis_code&quot;) qtm(munip, fill = &quot;hoofdcluster_lbl&quot;, fill.title = &quot;&quot;) Ook bij ruimtelijke variabelen merken we een autocorrelatie op, waarbij de waarde op één plaats niet onafhankelijk is van de waarde op een naburige plaats. 3.14 Toegang tot data Je moet natuurlijk zien toegang te krijgen tot de data. Als datawetenschapper moet je alvorens een opdracht te aanvaarden eerst dé vraag stellen. Net als Jerry Maguire moet je de vraag durven stellen: Show me the data! Soms zijn de data publiek toegankelijk en is het gewoon een kwestie om tijd te nemen om door de data te lopen, i.e. aan exploratie te doen. Soms moeten de data nog verzameld worden. Dan wil de datawetenschapper ook inspraak hebben in het verzamelen en de eventuele verwerking van de gegevens. Het kan natuurlijk voorkomen dat men enkel toegang verschaft nadat er een akkoord is om een ML project op te starten. Het advies is hier om er alles aan te doen om toch op zijn minst een glimp te kunnen opvangen van de data. Soms zijn de data te koop en krijg je geen data te zien alvorens je betaalt? Wel, indien je op voorhand geen toegang krijgt tot zelfs een klein representatief deel van de gegevensberg of -stroom, dan kan je nog altijd een samenvatting opvragen, het zogenaamd codeboek. Je kan de opdrachtgever belagen met vragen: Hoeveel tabellen zijn er? Voor elke tabel, hoeveel rijen en kolommen zijn er? Hoeveel data ontbreekt er? Wat is het formaat van de gegevens? enz… 3.15 Het codeboek Het eerste contact met een dataset verloopt normaal gezien via het codeboek. Het codeboek bevat de sleutel tot het begrijpen van de data zonder de data zelf te moeten inkijken. Hier is een voorbeeld in JSON formaat: { &quot;title&quot;: &quot;Occupancy Detection Data Set&quot;, &quot;description&quot;: &quot;Experimental data used for binary classification (room occupancy) from Temperature, Humidity, Light and CO2. Ground-truth occupancy was obtained from time stamped pictures that were taken every minute.&quot;, &quot;data_set_type&quot;: [&quot;Multivariate&quot;, &quot;Time-Series&quot;], &quot;instances_count&quot;: 20560, &quot;publication_date&quot;: &quot;2016-02-29&quot;, &quot;origin_contact_email&quot;: &quot;Luis Candanedo &lt;luismiguel.candanedoibarra@umons.ac.be&gt;&quot;, &quot;outcome&quot;: &quot;Occupancy&quot;, &quot;outcome_type&quot;: &quot;Binary&quot;, &quot;feature_type&quot;: &quot;Real&quot;, &quot;feature_count&quot;: 7, &quot;variable_descriptions&quot;: { &quot;Time&quot;: &quot;Time of measurement&quot;, &quot;Temperature&quot;: &quot;Temperature in Celsius&quot;, &quot;Humidity&quot;: &quot;Relative air humidity in %&quot;, &quot;Light&quot;: &quot;Light intensity in Lux&quot;, &quot;CO2&quot;: &quot;CO2 concentration in ppm&quot;, &quot;Humidity_Ratio&quot;: &quot;Derived quantity from temperature and relative humidity, in kg[water-vapor]/kg[air]&quot;, &quot;Occupancy&quot;: &quot;0 or 1, 0 for not occupied, 1 for occupied status&quot; }, &quot;references&quot;: [ &quot;Candanedo, L. M., &amp; Feldheim, V. (2016). Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Energy and Buildings, 112, 28-39.&quot; ] } Dit voorbeeld van een codeboek is oorspronkelijk afkomstig van de UCI repository. Het is daarna nog hier en daar aangepast om als voorbeeld te kunnen dienen. Dat het in JSON formaat staat is niet onbelangrijk. Soms gaat het over complexe datasets met duizenden kolommen. Zo bestaat de learning databank, onderliggend aan onze Digitap omgeving, uit 4992 kolommen! Als je dan op zoek moet naar de exacte eenheid waarin een variabele beschreven staat of wat een variabele nu weer precies betekent is dat niet werkbaar. Uit de het bovenstaande codeboek lezen we dat het gaat over sensordata in een ruimte om te proberen voorspellen of er iemand in de ruimte aanwezig is of niet. We zien dat er 20 560 instanties zijn, waarbij op elke instantie gegevens verzameld worden van de verscheidene sensoren. Dat is zeker niet slecht. Om aan statistiek te doen (ML behoort voor 100% tot de statistiek) moet je voldoende gegevens hebben en als de patronen in de gegevens maar amper ‘boven de ruis’ uitsteken, dan heb je méér data nodig. Daaronder lezen we dat de data publiek zijn gemaakt op 2016-02-29. Dit strookt met wat we later zouden ontdekken, namelijk dat de datums in de dataset vóór deze datum liggen (2015-02-02 14:19, …). Het wetenschappelijk artikel werd gepubliceerd in 2016, dus dat kan ook kloppen. Door op deze wijze een soort reconstructie te doen van de feiten kan een datawetenschapper soms achterhalen of de data nog wel hedendaags zijn of dat er bijvoorbeeld iets niet pluis is. Dit kom je niet vaak tegen, maar idealiter is het goed om, zoals hier, over de contactgegevens te beschikken voor in geval er toch nog vragen opduiken. Er wordt dus ook verwezen naar het wetenschappelijk artikel en daar vind je normaal gezien een detail-beschrijving van de methodologie. Zo vind je daar terug dat één van de gebruikte sensoren een vochtigheid- en temperatuur-sensor is van het type DHT22, informatie die van pas kan komen indien deze data anomalieën zouden vertonen. Tenslotte vinden we natuurlijk in het codeboek ook een beschrijving van de variabelen. Deze beschrijving omvat idealiter de volgende elementen: een beschrijving van de variabele de eventuele eenheden waarmee de grootheden geassocieerd dienen te worden of de variabele het gevolg is van een rechtstreekst meting (bijv. output van een sensor) of achteraf berekend werd uit andere variabelen zoals hier het geval is voor Humidity_Ratio een overzicht van de waarde ruimte, de set van mogelijke waarden die de variabele kan aannemen (\\(\\mathbb{R}\\), \\(\\mathbb{Z}^+\\), {0, 1}, datum-veld met resolutie van 1 minuut, …) Bronvermelding "],
["data-exploratie.html", "Hoofdstuk 4 Data exploratie 4.1 Principes van data exploratie 4.2 Stappen in data exploratie 4.3 Voorbeeld data exploratie 4.4 Univariate verdelingen 4.5 Correlatie tussen twee variabelen", " Hoofdstuk 4 Data exploratie Leerdoel 4.1 Past de juiste principes toe tijdens het exploreren, hanteren en opkuisen van data (EA_LD758). Leerdoel 4.2 Beeld complexe data uit door middel van hedendaagse visualisatie tools (EA_LD761). 4.1 Principes van data exploratie Exploratieve Data Analyse (eng: Exploratory Data Analysis; EDA) zal de datawetenschapper helpen om twee hoofddoelen te bereiken Stelling 4.1 De twee hoofddoelen die de datawetenschapper voor ogen heeft tijden het exploreren van data zijn Begrijpen van de data én van processen die tot de data geleid hebben Controleren dat de dataset in zijn geheel maar ook elke variabele afzonderlijk aan een aantal eisen voldoet De datawetenschapper wil tijdens het controleren van een variabele de volgende eigenschappen onderzoeken: D uidelijkheidKan de betekenis van de variabele achterhaald worden? Zo niet, dan is het vaak veiliger de variabele niet te gebruiken. I nformatiefIs de informatie inhoud voldoende groot. Bijvoorbeeld, is het erg repetitieve data, dan is het misschien niet erg bruikbaar. Wordt mede bepaald door voorradigheid (zie hieronder). P rivacyLoop je met de betrokken variabele het risico om de privacy van de eindgebruiker te schenden volgens de opgestelde Privacy Impact Assessment (PIA) of volgens de Vlaamse of Federale wetgeving, dan mag je deze variabele simpelweg niet ontsluiten. K waliteitZijn er technische redenen om de data niet te kunnen gebruiken? Zijn de data moeilijk om te onttrekken (bij bijv. vrije tekst), is er onvoldoende resolutie (bij metingen of afbeeldingen), gaat het om een té complex data type (bijv. circulaire), … E thiekHet trainen met tendentieuze data leidt tot een tendentieus algoritme. Om dit te voorkomen, laat je best variabelen weg die mogelijk wel een zeker voorspellend vermogen hebben en privacy-gewijs perfect toelaatbaar zijn, maar die de vooringenomenheid uit het verleden zou bevestigen. V oorradigOntbreekt er méér dan ±30% van de waarden voor een bepaalde variabele (NULL, NA, \"\", -1, …), dan kunnen vele leeralgortimen de data niet gebruiken. E chtheidHet kan voorkomen dat de data weinig betrouwbaar lijken in de zin dat ze vermoedelijk geen goede weerspiegeling zijn van de realiteit, i.e. ofwel vermoedelijk inaccurate data of niet representatief (bijv. slechts geldig voor beperkte subset). R elevantieGegeven de doelstelling van het onderzoek is het goed om je af te vragen of er wel een goede verantwoording (technisch noch functioneel) gevonden kan worden waarom deze variabele zou worden opgenomen. Hou hier best rekeneing met alle belanghebben zoals opdrachtgevers en eindgebruikers. 4.2 Stappen in data exploratie Elke exploratie is anders en iedereen heeft zo zijn eigen methode, maar als leidraad kan je deze stappen volgen: Zorg eerst dat je zo veel mogelijk begrijpt van de data nog vóór je in detail in de variabelen begint te duiken. Lees het codeboek of andere omschrijvingen. Vraag indien mogelijk hulp van iemand die al wel op de hoogte is Controleer of er tabellen, rijen of kolommen zijn die je niet nodig hebt voor jouw analyse en filter de overbodige data desgevallend weg zodat je hier ook geen tijd mee hoeft in te steken Begrijp wat één instantie (rij) hier precies voor staat en controleer of het aantal rijen dan ook logisch is Controleer nu pas de overblijvende data variabele-per-variabele Lees de omschrijving van de variabele en kijk naar een samenvatting of een willekeurige steekproef van de variabele Loop de DIPKEVER criteria één voor één af Beslis op basis hiervan of je de variabele wenst te houden (eng: keep) of te verwerpen (eng: drop) Maak korte notities Kijk naar de paarsgewijze correlaties (eng: pair-wise correlations) onder de variabelen 4.3 Voorbeeld data exploratie We gaan dit doen aan de hand van een voorbeeld (in R). Voorbeeld 4.1 Vraag: Sommige mensen zeggen dat er steeds méér terrorisme is in de wereld. Andere zeggen dat er altijd al terrorisme heeft bestaan. Wie heeft er gelijk? Antwoord: Via de bovenstaande link naar de standaard datasets vinden we twee datasets met het woordje “terrorism” in de titel. Ééntje heeft 206 rijen (nkill.byCountryYr), laten we die eens van naderbij bekijken: library(Ecdat) library(magrittr) ?terrorism Zorg er eerst voor dat je deze pakketten installeert (eenmalig) via install.packages of via de Packages-tab in RStudio. Als alles goed verlopen is, krijg je de documentatie te zien: We onderzoeken nu de documentatie en vinden dat de databank wel erg relevant en professioneel lijkt, maar niet zo up-to-date (1970 - 2015). Toch beslissen we om door te gaan met deze dataset. We lezen dat er eigenlijk drie datasets zijn, telkens met hun eigen structuur: terrorism: Jaartal × {verscheidene aantallen per type aanslag, locatie (wereld versus VS) en type slachtoffers} incidents.byCountryYr: Aantal aanslagen per {Land × Jaartal} nkill.byCountryYr: Aantal slachtoffers per {Land × Jaartal} Hierop lanceren we de vraag: Vraag terug: met ‘méér terrorisme’, bedoel je méér aanslagen of méér slachtoffers Stel dat de opdrachtgever het volgende antwoord geeft: Antwoord terug: méér aanslagen We kunnen nu zowel de eerste als de tweede gebruiken. We kiezen de eerste omdat daarin meer informatie vervat zit. We hebben die extra informatie nu niet nodig, maar misschien later wel. Één instantie (een rij) komt overeen met een jaartal. We zien onmiddellijk dat we de volgende variabelen nodig hebben uit de terrorism dataset: year: het jaar waarin de aanslag werd gepleegd, als natuurlijk getal, uniek voor elke rij incidents: het aantal aanslagen voor dat jaar wereldwijd dat &lt;- terrorism %&gt;% as.data.table %&gt;% extract(, .(year, incidents)) We overlopen de DIPKEVER criteria. De data zijn duidelijk en informatief. Ze lijken de privacy van niemand te schenden omdat het om geaggregeerde gegevens gaat, wereldwijd en telkens samengevat voor een heel kalenderjaar. Kwaliteit. We beginnen met de controle dat het jaartal en de incidenten als natuurlijke getallen worden behandeld. dat %&gt;% apply(2, class) ## year incidents ## &quot;numeric&quot; &quot;numeric&quot; numeric datatype in R stellen de reële getallen voor, niet natuurlijke getallen. Dat is geen ramp, natuurlijk, omdat \\(\\mathbb{N}\\subset\\mathbb{R}\\) maar kan soms problemen geven, laten we dit rechtzetten: dat[, year := year %&gt;% as.integer] dat[, incidents := incidents %&gt;% as.integer] dat %&gt;% apply(2, class) ## year incidents ## &quot;integer&quot; &quot;integer&quot; Nu controleren we of het jaartal inderdaad loopt van 1970 tot en met 2015: dat$year %&gt;% summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1970 1981 1992 1992 2004 2015 en we kunnen ook nakijken of het jaartal uniek is: dat$year %&gt;% duplicated %&gt;% any ## [1] FALSE De incidenten zijn ook natuurlijke getallen, maar van een andere soort dan het jaartal. De incidenten zijn namelijk ook aantallen en dit heeft heel wat statistische consequenties. Het betekent onder meer dat we voor deze variabele een histogram kunnen genereren: dat$incidents %&gt;% hist(main = &quot;Verdeling aantal aanslagen&quot;, xlab = &quot;# aanslagen per jaar&quot;, ylab = &quot;Frequentie&quot;) Hierin zien we dat er in het totaal meer dan 30 jaren zijn met minder dan 5000 aanslagen per jaar wereldwijd en dat er 3 jaren zijn met meer dan 10 000 aanslagen. Dit was de K van DIPKEVER, we zetten onze tocht voort. Over ethiek: voor velen is dit natuurlijk een gevoelig onderwerp maar het weergeven van de aantallen aanslagen kan moeilijk als aanstootgevend worden gezien. Bovendien worden hier nog geen voorspellingen gemaakt, dus dat lijkt in orde. Er ontbreken geen data (dus voorradig). Het consortium rond de Studie van Terrorisme en Responsen op Terrorisme (START) is verantwoordelijk voor de data en gezien hun staat van dienst en academische publicaties waaronder sommige in gerenommeerde tijdschriften, lijken ze op het eerste gezicht betrouwbaar (cfr. Echtheid). Tenslotte zijn de data ongetwijfeld relevant en daarmee slagen ze voor de volledige DIPKEVER test. Het onderlinge correleren van beide variabelen biedt hier geen meerwaarde. De exploratiefase is voorbij. Het volstaat ons nu om een grafiek op stellen waarbij het aantal aanslagen in de tijd wordt gevolgd: dat &lt;- dat[order(year)] void &lt;- dat[, plot(incidents ~ year, type = &quot;l&quot;, main = &quot;Verloop aantal aanslagen wereldwijd&quot;, xlab = &quot;Kalenderjaar&quot;, ylab = &quot;Aantal aanslagen per jaar&quot;)] Conclusie: Ja, het aantal gerapporteerde terroristische aanslagen lijkt volgens de START (Pape et al. (2014)) toegenomen te zijn in de periode 2010 - 2015 ten opzichte van de periode 1970 - 2010. 4.4 Univariate verdelingen We zagen net hoe je met een eenvoudige opdracht een histogram kunt opstellen voor een variabele. Het histogram, geïntroduceerd in de paragraaf rond Soorten variabelen, geeft de verdeling weer van de waarden van één variabele (univariaat, eng: univariate) langsheen de meetschaal van de variabele. Dat is cruciale informatie. Één blik op de verdeling geeft antwoord op de volgende vragen: Wat is het datatype? Lijkt het een gekende theoretische verdeling te volgen? Heeft een duidelijke modus, meerdere modi? Lijkt het erop dat de data een natuurlijke oorsprong kennen of is de data mogelijk gemanipuleerd? Wat is de resolutie van de meting? Lijken er limieten te zijn waartussen de waarden zich bevinden Voorbeeld 4.2 Laten we de dataset rhDNase van naderbij onderzoeken met als doel om uitsluitend naar de verdelingen te kijken. We beginnen met het laden van de dataset en het tonen van de interne structuur (str) van het object dat de data bevat library(survival) data(rhDNase) rhDNase %&gt;% str ## &#39;data.frame&#39;: 767 obs. of 8 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ inst : int 1 1 1 1 1 1 1 1 2 2 ... ## $ trt : int 1 1 0 1 0 1 0 0 1 0 ... ## $ entry.dt: Date, format: &quot;1992-03-20&quot; &quot;1992-03-24&quot; &quot;1992-03-24&quot; &quot;1992-03-26&quot; ... ## $ end.dt : Date, format: &quot;1992-09-04&quot; &quot;1992-09-09&quot; &quot;1992-09-08&quot; &quot;1992-09-10&quot; ... ## $ fev : num 28.8 64 67.2 57.6 57.6 25.6 86.4 32 86.4 28.8 ... ## $ ivstart : num NA NA 65 NA NA NA NA 90 NA 8 ... ## $ ivstop : num NA NA 75 NA NA NA NA 104 NA 22 ... We gaan nu één voor één de variabelen af: id: subject id. Is de identiteit en het heeft geen nut om hier een verdeling van de trekken. inst: enrolling institution. Dit is een integer, dus dat lijkt niet te kloppen, want we verwachten een organisatie hier. We behandelen het dan ook als categorische (nominale) variabele (factor in R) en maken een staafdiagram (eng: bar plot). Er is geen betekenisvolle sortering mogelijk bij categorische variabelen, maar het is een goed gewoonte om te toch even van frequent naar minder frequent te sorteren om een zicht te krijgen op de curve die zo ontstaat. rhDNase &lt;- rhDNase %&gt;% as.data.table rhDNase[, inst := inst %&gt;% factor] rhDNase$inst %&gt;% table %&gt;% sort %&gt;% rev %&gt;% barplot(xlab = &quot;Enrolling institution&quot;, ylab = &quot;# of instances&quot;, main = &quot;Distribution of the institutions&quot;, cex.names = .6, las = 2) Er zijn wel een aantal dominantere instituten maar die dominantie lijkt niet te extreem er blijft een goede verdeling tussen de verschillende instituten van grofweg 10-15 instanties per instituut. trt treatment arm: 0=placebo, 1= rhDNase Een arm betekent hier een vertakking van de beslissingsboom die gebruikt wordt om patiënten in een bepaalde categorie te plaatsen. Zulke een binaire kan je evengoed met een staafdiagram voorstellen, maar hier heb je duidelijk een controle experiment (de placebo-arm) en volstaat het om de verhouding behandeld/placebo mee te geven omdat alle informatie van de verdeling hierin vervat zit: rhDNase$trt %&gt;% sum %&gt;% divide_by(rhDNase %&gt;% nrow) %&gt;% multiply_by(100) %&gt;% sprintf(&quot;%.1f%%&quot;, .) ## [1] &quot;48.8%&quot; Dit ligt héél dicht bij 50% en we kunnen dus zeggen dat de groepen mooi gebalanceerd zijn zoals het hoort entry.dt: date of entry into the study. Uit de structuur zagen we al dat het formaat correct in ingesteld als datum. Een datum is in theorie een continue variabele omdat er tussen twee willekeurig gekozen tijdstippen oneindig veel nieuwe tijdstippen liggen. Bovendien heeft een datum geen betekenisvol nulpunt. Voor een continue variabele is een densiteit-diagram beter geschikt dan een histogram: rhDNase[, entry.dt.num := entry.dt %&gt;% as.numeric] rhDNase$entry.dt.num %&gt;% density %&gt;% plot(xaxt = &quot;n&quot;, main = &quot;Distribution of start dates &quot;, xlab = &quot;&quot;) axis(1, las = 2, cex = .8, at = rhDNase$entry.dt %&gt;% pretty, labels = rhDNase$entry.dt %&gt;% pretty) rhDNase$entry.dt.num %&gt;% rug Hier zien we dat de rekrutering van deelnemers voor de klinische studie na één maand op een laag pitje te hebben gestaan pas goed op gang is gekomen. De densiteit laat twee pieken zien (modi), we spreken van een bimodale verdeling. end.dt: date of last follow-up. Is sterk gelijkend op het vorige. Laten we deze feature gebruiken om de loopduur van elke instantie te berekenen en de distributie hiervan in grafiek te zetten (zie Oefening 4.1). Oefening 4.1 (Distributie-loopduur) Probeer de distributie voor de loopduur per instantie in beeld te brengen en te beschrijven wat je ziet. ivstart en ivstopdays from enrollment to the start of IV antibiotics en days from enrollment to the cessation of IV antibiotics (IV staat voor intraveneus) Deze laatste twee variabelen zijn weer verschillend: het gaat om discrete aantallen (\\(\\subset \\mathbb{N}\\)), dus geen kommagetallen, maar, tegen de verwachting, wél negatief kunnen zijn. Bovendien zijn er erg veel ontbrekende waarden: rhDNase$ivstart %&gt;% head(32) ## [1] NA NA 65 NA NA NA NA 90 NA 8 63 60 83 50 NA 99 35 71 NA 8 NA NA NA 20 62 85 NA 13 ## [29] 51 166 37 65 rhDNase$ivstart %&gt;% is.na %&gt;% table ## . ## FALSE TRUE ## 367 400 We kunnen zelfs zeggen dat er voor ivstart meer gaten zijn dan kaas. We kunnen alsnog de verdeling onderzoeken van de overblijvende waarden: rhDNase$ivstart %&gt;% hist( main = &quot;Verdeling wachttijd voor behandeling&quot;, xlab = &quot;wachttijd&quot;, ylab = &quot;# instanties&quot;) abline(v = 0, lwd = 2) We zien een verdeling die eerder uniform is dan wel normaal. Ook hier kunnen we nu de verdeling van de behandelingsduur in grafiek zetten: par(mfrow = 1:2) rhDNase[, ivstop - ivstart] %&gt;% hist (main = &quot;Behandelingsduur&quot;) rhDNase[, ivstop - ivstart] %&gt;% log %&gt;% hist(main = &quot;ln(Behandelingsduur)&quot;) De eerste grafiek laat zien dat de behandelingsduur nogal rechtsscheef verdeeld is. De transformatie door middel van de natuurlijke logaritme (\\(ln(x)=\\,^elog(x)\\)) veranderd de verdeling in iets wat normaler lijkt. We zeggen dat de behandelingsduur lognormaal verdeeld lijkt te zijn. Laten we de theoretische curve erbij zetten om de afwijkingen tussen werkelijk en theoretisch te visualiseren: bduur_log &lt;- rhDNase[, log(ivstop - ivstart)] bduur_log &lt;- bduur_log[!is.na(bduur_log) &amp; !is.infinite(bduur_log)] bduur_log %&gt;% hist(main = &quot;ln(Behandelingsduur)&quot;, freq = FALSE, n = 20) x &lt;- (1:60 / 10) y &lt;- x %&gt;% dnorm( mean = bduur_log %&gt;% mean, sd = bduur_log %&gt;% sd) lines(x, y , col = 1, lwd = 3) We kunnen verdelingen ook met andere grafiek-types in beeld brengen. De meest populaire zijn de boxplot, de {dot plot}(https://en.wikipedia.org/wiki/Dot_plot_(statistics)), de beeswarm plot en de violin plot. Oefening 4.2 (Verdeling) Tracht de verdeling van de logaritmisch geschaalde behandelingsduur te visualiseren door middel van de functies base::dotchart, base::boxplot, beeswarm::beeswarm en vioplot::vioplot. De oplossing moet er zo uit zien: Eindresultaat grafieftypes Tip: Als je meer dan 300 karakters nodig hebt voor heel deze oefening, weet je dat je te ver aan het zoeken bent. Opgelet: de variabele bduur_log wordt voor deze oefening verwacht in scope te zijn. 4.5 Correlatie tussen twee variabelen Om correlatie te begrijpen simuleren we een bivariate verdeling met verschillende covariantie-matrices (zie figuur 4.1). library(MASS) library(latex2exp) copula &lt;- function(n, rho, mu1, mu2, sd1, sd2){ mu &lt;- c(mu1,mu2) sigma &lt;- matrix(c(sd1 ^ 2, sd1 * sd2 * rho, sd1 * sd2 * rho, sd2 ^ 2),2) mvrnorm(n, mu, sigma) %&gt;% as.data.table } par(mfrow = c(2, 2), mar = c(2, 3, 2, 1)) for(rho in c(.05, .33, .66, .95)){ dat &lt;- copula(1000, rho, 0, 0, 1, 1) dat %&gt;% plot (xlab = &quot;&quot;, ylab = &quot;&quot;, pch = 19, cex = .7, xlim = c(-3, 3), ylim = c(-3, 3)) text(-2.5, 2.7, TeX(sprintf(&quot;$\\\\rho = %.2f$&quot;, rho))) text(2, -2.7, TeX(sprintf(&quot;$\\\\hat{\\\\rho} = %.4f$&quot;, dat %&gt;% cor %&gt;% extract(1, 2)))) } Figuur 4.1: Demonstratie van de correlatie tussen twee variabelen. Merk op hoe de geschatte correlatie coëfficiënt de werkelijke benaderd. De covariantie matrix komt overeen met (\\(\\rho\\) is de correlatiecoëfficiënt) \\[\\begin{bmatrix}1 &amp; \\rho\\\\\\rho &amp; 1\\end{bmatrix}\\] Opmerking Opgelet: een correlatie tussen twee variabelen is geen garantie voor een causaal verband. Bekijk deze kennis-clip om te begrijpen hoe je wel een causaal verband kan onderzoeken: Oefening 4.3 (Wanneer corelleer) Gegeven de volgende combinaties van verschillende soorten variabelen, wanneer mag je wél correleren: Nominaal vs numeriek discreet lineair Numeriek continu lineair vs Numeriek continu lineair Nominaal vs numeriek continu circulair Numeriek continu lineair vs numeriek continu circulair Numeriek continu lineair vs ordinaal Bronvermelding "],
["manipuleren-van-data.html", "Hoofdstuk 5 Manipuleren van data 5.1 Kort overzicht van de manipulaties 5.2 (Eng.) Changing Category Names 5.3 (Eng.) Imputation 5.4 Onbehandeld", " Hoofdstuk 5 Manipuleren van data Leerdoel 5.1 Past de juiste principes toe tijdens het exploreren, hanteren en opkuisen van data (EA_LD758). Hieronder volgt een korte samenvatting van een aantal vaak gebruikte transformaties. Waarom zou je data willen transformeren. Hier staan een aantal redenen opgesomd: Om duidelijkheid te scheppen en de data leesbaarder te maken voor mensen Om de data voor te bereiden voor statische analyse Om nieuwe variabelen te creëren (eng: feature engineering) Om eenvoudiger gedistribueerd en parallel te kunnen programmeren Om de data te standaardiseren volgens CDMs (common data models) van het bedrijf (zoals voor een EDM Enterprise Data Model) of van een discipline (zoals CDISC standaarden in de klinische wereld) Om de data te onderworpen aan de regels van een bepaalde type opslag software. Wil je bijvoorbeeld de analyse dataset bewaren in een relationele databank, dan zal je rekening moeten houden met de regels rond primary, surrogate en foreign keys. 5.1 Kort overzicht van de manipulaties 5.1.1 Filteren en versnijden (eng: subsetting, slicing) Het versnijden is hier te interpreteren in de betekenis van aan stukken van een bepaalde vorm snijden en niet bijvoorbeeld in de betekenis van aanmengen met iets van mindere kwaliteit. Het komt er op neer dat er een selectie van variabelen en instanties gemaakt wordt. Figuur 5.1: Schematisch overzicht van het versnijden van een dataset. Denk eraan dat het selecteren van van variabelen een courante activiteit is, terwijl het selecteren van instanties eerder zeldzaam is. Stelling 5.1 Het achterhouden of selecteren van variabelen, maar voornamelijke van instanties, kan vertrekkende gevolgen hebben. De datawetenschapper moet in het algemeen alle manipulaties maar filter-operaties in het bijzonder zoals het versnijden volledig en transparant rapporteren. Dus voorzichtig heid geboden. Maar in sommige gevallen is het verantwoord om ook instanties uit de dataset te filteren: De data bevat uitlopers die verantwoord kunnen worden of die zonder twijfel veroorzaakt worden door een foutieve invoer De combinatie van sommige variabelen voor een bepaalde instantie is heel waarschijnlijk foutief en is niet het gevolg van structurele maar eerder van eenmalig fouten in de dataset of het invoer-proces Een instantie bevat een combinatie van variabele waarden die de privacy van één of meerdere personen in het gedrang brengt Voorbeeld 5.1 (Versnijden van data) De wijn kwaliteit dataset bevat mogelijk outliers in de suikerwaarden, laten we dat eens nader bekijken. rode_wijn &lt;- fread(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&quot;) rode_wijn[, `residual sugar`] %&gt;% density %&gt;% plot (main=&quot;Residual sugar (g/L)&quot;) Stel dat rode wijn met een suikergehalte boven de 10g/L wordt afgedaan als fruitsap, dan zou je er van uit mogen gaan dat er een aantal uitlopers zijn, zo halen we ze eruit: aantal_uitlopers &lt;- rode_wijn[`residual sugar` &gt; 10, .N] rode_wijn_echt &lt;- rode_wijn[`residual sugar` &lt; 10] rode_wijn_echt$`residual sugar` %&gt;% density %&gt;% plot (main=&quot;Residual sugar (g/L)&quot;, sub = paste(&quot;Excluding&quot;, aantal_uitlopers, &quot;&#39;outliers&#39; above 10g/L&quot;)) 5.1.2 Booleaans masker Een booleaans masker is gewoon een vector of lijst van booleaanse waarden die gebruikt kunnen worden om uit data frames en data tables die rijen te selecteren die overeenkomen met TRUE in het masker. In het Voorbeeld 5.1 werd er reeds van een masker gebruik gemaakt: rode_wijn[,`residual sugar` &lt; 10] %&gt;% head(50) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [13] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [25] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE ## [37] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [49] TRUE TRUE 5.1.3 (Eng.) Grouping and Aggregation Upon Grouping the data table according to one or more features, the other variables need to be aggregated en this can be done using: Selection (first, last, second value, …) Measures of central tendency (mean, median, …) Measures of dispersion (variance, range, standard deviation, IQR, …) Extremes (min, max) Counts (Count, unique count, sum, majority vote, counts of missing values, …) 5.1.4 (Eng.) Transforming text Transformation Example Possible justification Change Encoding UTF16 → UTF8 Avoid software incompatibilities Change Casing \"tfr523\" → \"TFR523\" Avoid confusing and allow for proper comparison Trim \" TFR523\" → \" TFR523\" Ensure proper identification &amp; comparison Add Leading zeroes \"653\", \"61\", … → \"0653\", \"0061\", … Avoid str/num confusion by humans and machines Add prefix \"0653\" → \"UHR0653\" Avoid confusion with other IDs 5.1.5 (Eng.) Re-Scaling Numerical Values Here are some of the countless different types of re-scaling: Logarithm Logit-transform Normalization to a fixed maximum value Standardization (to a mean of 0 and a variance of 1) Division by another field (e.g. Quantity_By_Month / Days_Per_Month, Revenue_By_Store / Sold_By_Store, Quantity / Weight_Per_Unit, …) Multiplication (e.g. Quantity x Unit_Price, Quantity x Unit_Cost) Subtraction (e.g. Sales_value – Sales_cost) 5.1.6 (Eng.) Discretizations Is going from continuous data to more discrete data. Here are some examples: Rounding ([2.25, 2.35] → [2.2, 2.4]) Binning (here and here) Mind that discretization always leads to loss of information. 5.1.7 (Eng.) Information Content The information of a feature or an entire data set can be measured using indices such as the Shannon Entropy or the limiting density of discrete points. 5.1.8 (Eng.) Reformatting, Type Conversion, Casting or Coercion Examples: String to Integer or vice versa Reformatting date to ISO-8601 (\"07/04/2008 → \"2007-07-04\" (YYYY-MM-DD)) 5.1.9 (Eng.) Changing numerical Values Far more invasive then re-scaling is when the data is changed based on some custom algorithm. An example hereof is the removal of a drift in the data (de-trending). Similarly, seasonal effects in time series data can be compensated away. Such changes can obviously impact the outcome of the analysis significantly and needs thorough justification. 5.2 (Eng.) Changing Category Names This is sometimes called recoding or refactorization and only affects nominal features. It serves to adhere to EDM, to the standards of the DWH and to ensure compatibility across data sets. Alternatively, it simply ensures that misspelled category names are being corrected and alternative spellings are being merged. 5.3 (Eng.) Imputation Imputation is the completion of missing data. Depending of the patterns the ‘holes’ in the data display or on prior knowledge, separate techniques exist to fill in the blanks. 5.4 Onbehandeld De volgende onderwerpen werden niet behandeld, ik hoor het graag als jullie individueel of als groep hier meer over willen leren: Anonimiseren en pseudonimiseren "],
["de-percetron.html", "Hoofdstuk 6 De percetron 6.1 Historiek 6.2 De anatomie van de perceptron 6.3 Casestudy: Onderscheiden van setosa 6.4 De perceptron klasse 6.5 De perceptron functies 6.6 Het leeralgoritme van de perceptron 6.7 Trainen van de perceptron 6.8 Voorspellen van de iris soort", " Hoofdstuk 6 De percetron 6.1 Historiek De perceptron is gebaseerde op een wiskundige beschrijving van het dierlijk neuron. De uitvinding van de perceptron wordt toegekend aan Frank Rosenblatt die in 1957 een leeralgoritme programmeerde op een IBM 704 computer. Figuur 6.1: IBM 704 mainframe. bron: Lawrence Livermore National Laboratory (n.d.). In de New York Times van 8 Juli 1958 verscheen: The Navy revealed the embryo of an electronic computer today that it expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence. Al snel werd er hardware gemaakt om het algoritme efficiënt uit te voeren, de zogenaamde Mark I perceptron machine Figuur 6.2: De Mark I perceptron machine, waarbij een matrix van 20 bij 20 cadmium sulfide fotocellen verbonden werden met een reeks potentiometers die de gewichten voorzien van de 400 invoer noden. bron: Cornell University Library (n.d.). 6.2 De anatomie van de perceptron Figuur 6.3: De anatomie van het neuron en de overeenkomstige node van een neuraal netwerk. In het dierlijke neuron (boven) krijgt het cellichaam (eng: cell body) allerlei invoer van verscheidene locaties. In dit cellichaam vindt de integratie plaats (sommatie) en transformatie waarna het signaal (in de vorm van een zogenaamde actiepotentiaal) langsheen het axon reist naar het axon-uiteinde (eng: synaptic terminals). In een neuraal netwerk zijn de axon-uiteinden verbonden met de dendrieten (eng: dendrites) van een ander neuron. In een perceptron (onder) wordt de invoer (\\(x\\)) gewogen en geïntegreerd tot één getal (\\(z\\)) dat getransformeerd kan worden alvorens het wordt als uitvoer (\\(y\\)) wordt vrijgegeven. Bron: Commons (2013). Naar analogie met het biologisch neuron is de functie van een perceptron om complexe invoer om de ene of andere manier te reduceren tot één getal (\\(z\\); zie Formule (6.1)). Dit getal wordt soms de logit van een neuron genoemd. \\[\\begin{equation} \\mathbb{R}^n\\rightarrow\\mathbb{R}\\\\ z=g(x, \\theta)=\\sum_{i=0}^{n}{\\theta_ix_i} \\tag{6.1} \\end{equation}\\] In een tweede functie, de zogenaamde activatiefunctie (eng: activation function), wordt dit resultaat nog eens getransformeerd tot de voorspelling \\(\\hat{y}\\). Één voorbeeld van zulk een transformatie is het signum (eng:sign function): \\[\\begin{equation} \\hat{y}=t(z)= \\begin{cases} -1\\,als\\,z&lt;0\\\\ 1\\,als\\,z\\geq0 \\end{cases} \\tag{6.2} \\end{equation}\\] Er zijn tal van andere transformaties mogelijk.zoals de heaviside-functie die nul geeft indien \\(z&lt;0\\) en 1 in alle andere gevallen. 6.3 Casestudy: Onderscheiden van setosa De beste manier vaak om iets te begrijpen is om de code te zien, toch? Op basis van een vereenvoudigd voorbeeld gebaseerd op de blog van Simone Alberto Peirone (Peirone (2007)) gaan we een leeralgoritme uit het niets opbouwen. Dit kan hier alleen omdat het om een uiterst eenvoudig leeralgoritme gaat, de perceptron. In de beroemde iris dataset worden een aantal eigenschappen van drie sterk gelijkende iris soorten verzameld. We kwamen deze dataset eerder al tegen in het hoofdstuk rond model complexiteit. Figuur 6.4: De drie soort van de iris dataset. Bron: Szczecinkowaty (2007), Mayfield (2005), Mayfield (2007). Als we twee eigenschappen van de iris bloemen, namelijk de lengte (\\(l\\)) en breedte (\\(b\\)) van het kelkblad, tegenover elkaar uitzetten, dan kunnen we de setosa soort van de andere twee onderscheiden: data(iris) plot(Sepal.Length ~ Sepal.Width, data = iris, pch = 19, cex = .8, col = 1 + (Species == &quot;setosa&quot;), xlab = expression(&quot;Kelkblad breedte &quot;~italic(b)~&quot; (mm)&quot;), ylab = expression(&quot;Kelkblad lengte &quot;~italic(l)~&quot; (mm)&quot;), main = &quot;Setosa vs {versicolor, virginica}&quot;) legend(&quot;topright&quot;, c(&quot;{versicolor, virginica}&quot;, &quot;setosa&quot;), col = 1:2, pch = 19) Figuur 6.5: Twee eigenschappen van iris bloemen (kelkblad lengte en breedte) tegenover elkaar uitgezet om een onderscheid te maken tussen Iris setosa en de twee andere soorten Iris versicolor en Iris virginica. In dit twee-dimensionaal geval zouden we dat evengoed manueel kunnen doen, maar dit geldt niet voor hogere dimensies (i.e. méér variabelen). Daarom gaan we alsnog gebruik maken van een perceptron. We doen dit omdat het perceptron natuurlijk de eenvoudigste vorm van een neuraal netwerk is, maar weet dat een perceptron slechts in een beperkt aantal gevallen geschikt is. Stelling 6.1 De perceptron is een leeralgoritme dat enkel geschikt is voor het onderscheiden van twee klassen (binaire outcome) en indien beide klassen lineair onderscheidbaar zijn (eng: linearly separable). 6.4 De perceptron klasse We gaan de perceptron in Python coderen, terwijl de resultaten in R zullen worden onderzocht (zie de § The R Language in de Appendix om te begrijpen waarom). import numpy as np class Perceptron: def __init__(self, learning_rate = 0.1, iteration_count = 50): self.learning_rate = learning_rate self.iteration_count = iteration_count self.theta_0 = 0.0 self.theta = None self.errs = [] De Perceptron klasse wordt geïnitialiseerd met de hyperparameters learning_rate en n-iter. De learning_rate (nl: leersnelheid) is een correctiefactor waarmee de gewichten \\(\\theta_i\\) worden vermenigvuldigd indien de uitkomst van de instantie fout is. iteration_count geeft aan hoeveel cycli het leeralgoritme moet volbrengen. theta_0 en theta zijn de parameters en theta_0 is een constante \\(\\theta_0\\) die aan de formule (6.1)) kan worden toegevoegd (zie Formule (6.3) en komt in het tweedimensionaal geval neer op een asafsnede (eng: intercept). Deze constante wordt echter meestal weggelaten omdat het erop neerkomt dat een extra berekende variabele \\(x_0\\) wordt toegevoegd, bestaande uit allemaal enen, waarvoor dan een overeenkomstige \\(\\theta_0\\) wordt voorzien. \\[\\begin{equation} g(x,\\theta)=\\theta_0+\\sum_{i=1}^{n}{\\theta_ix_i} \\tag{6.3} \\end{equation}\\] In de interne variabele errs worden het aantal fouten bewaard die bij elke cyclus gemaakt worden. Hiermee kunnen we het verloop van de performantie van het leeralgoritme opvolgen of achteraf reconstrueren. Hoewel in bovenstaande code hier nog geen gebruik van wordt gemaakt, zorgt de geïmporteerde numpy module (onder alias np) ervoor dat er vlot met vectoren en matrices gerekend kan worden. 6.5 De perceptron functies We zagen twee functies die de eigenlijke kern vormen van de perceptron: \\(g\\) en \\(t\\). Laten we deze als methoden toevoegen: class Perceptron: [...] def g(self, x: np.array) -&gt; float: return np.dot(x, self.theta) + self.theta_0 def t(self, z: float) -&gt; int: return np.where(z &gt;= 0.0, 1, -1) Met de np.dot functie berekenen we het inwendig product tussen de matrix \\(x\\) en de vector \\(\\theta\\). Zie Appendix voor meer uitleg hierover. 6.6 Het leeralgoritme van de perceptron De rest van de noodzakelijke code is a.h.w. van administratieve aard: def fit(self, x: np.array, y: np.array): self.theta = np.zeros(x.shape[1]) for iteration in range(self.iteration_count): err_iter = 0 for xi, yi in zip(x, y): update = self.learning_rate * (yi - self.t(self.g(xi))) self.theta_0 += update self.theta += update * xi err_iter += int(update != 0.0) self.errs.append(err_iter) Laten we de code even overlopen. self.theta = np.zeros(x.shape[1]) Voor elke instantie in de invoer matrix \\(x\\) wordt er een gewicht \\(\\theta_i\\) voorzien en op nul geïnitialiseerd. shape geeft de dimensies terug van een matrix en de eerste waarde geeft het aantal rijen. for iteration in range(self.iteration_count): err_iter = 0 for xi, yi in zip(x, y): # Updaten van de gewichten en aantal fouten tellen self.errs.append(err_iter) Tijdens elke cyclus wordt het aantal foute voorspellingen binnen de cyclus bijgehouden. De zip functie plakt telkens de werkelijke uitkomst aan een instantie. In ons geval zal \\(x_1\\) bestaan uit de breedte van de kelkbladeren van de irissen en \\(x_2\\) de lengte van de kelkbladeren bevatten. Dus, stel breedte = 3.5mm, lengte = 5.1mm en soort = setosa, dan verwachten we de eerste keer dat de compiler in de lus python for xi, yi in zip(x, y): terecht komt de volgende waarden: xi = [3.5, 5.1] yi = 1 Binnen in deze lus, wordt er berekend met hoeveel de parameters moeten worden verminderd: update = self.learning_rate * (yi - self.t(self.g(xi))) Stel dat het leeralgoritme voor een bepaalde instantie 1 voorspelt, maar dat de werkelijke uitkomst -1 is, dan worden de parameters vermeerderd met de fractie hetgeen overeenkomt met \\(0.1\\cdot\\left(-1\\cdot-1\\right)=-0.2\\) voor een leersnelheid van \\(0.1\\). Met andere woorden, de parameters worden verminderd met één vijfde van hun waarde. Stel nu dat het leeralgoritme het juist had, dan wordt update gelijkgesteld aan \\(0.1\\cdot\\left(1\\cdot-1\\right)=0\\) en blijven de parameters onveranderd. En dit was het! Hier is de volledige werkende Python code: import numpy as np class Perceptron: def __init__(self, learning_rate = 0.1, iteration_count = 10): self.learning_rate = learning_rate self.iteration_count = iteration_count self.theta_0 = 0.0 self.theta = None self.errs = [] def g(self, x: np.array) -&gt; float: return np.dot(x, self.theta) + self.theta_0 def t(self, z: float) -&gt; int: return np.where(z &gt;= 0.0, 1, -1) def fit(self, x: np.array, y: np.array): self.theta = np.zeros(x.shape[1]) for iteration in range(self.iteration_count): err_iter = 0 for xi, yi in zip(x, y): update = self.learning_rate * (yi - self.t(self.g(xi))) self.theta_0 += update self.theta += update * xi err_iter += int(update != 0.0) self.errs.append(err_iter) 6.7 Trainen van de perceptron De tijd is aangebroken om de perceptron te trainen met de data van de iris bloemen. Het enige wat we nog willen doen is de invoergegevens standaardiseren, i.e. de zogenaamde Z-score berekenen. Let wel deze Z heeft niets te maken met de \\(z\\) (de logit) die we eerder zagen. \\[\\begin{equation} Z=\\frac{x_j-\\mu_j}{\\sigma_j} \\tag{6.4} \\end{equation}\\] Elke variabele \\(x_j\\) wordt in dit process eerst verminderd met het gemiddelde voor die variabele \\(\\mu_j\\) en daarna gedeeld door de standaardafwijking van die variabele \\(\\sigma_j\\). Wat de Z-score in feite doet is elke waarde transformeren naar het aantal standaardafwijkingen het verwijderd is van het gemiddelde. De reden dat we de standaardisatie uitvoeren is omdat de lengte en de breedte van de kelkbladen een andere schaal hebben en daar kan dit eenvoudige leeralgoritme moeilijk mee overweg. Maar opgelet, het nemen van het gemiddelde en de standaardafwijking van een feature veronderstelt dat deze zich (toch ongeveer) normaal verdeeld. In de paragraaf over Univariate verdelingen staat beschreven hoe we een verdeling snel visueel kunnen controleren: par(mfrow = 1:2) iris$Sepal.Width %&gt;% density %&gt;% plot (main = &quot;Kelkblad breedte&quot;) iris$Sepal.Length %&gt;% density %&gt;% plot (main = &quot;Kelkblad lengte&quot;) Toegegeven, de verdelingen wijken enigszins af van de normaalverdeling, maar het belangrijkste is dat beide variabelen ten minste unimodaal zijn, i.e. één ‘piek’ bezitten in hun verdeling. Dus een standaardisatie lijkt gelegitimeerd. De onderstaande code verzamelt de invoer en de uitkomsten en voert de standaardisatie door in R met de functie base::scale: x_all &lt;- iris[, c(&quot;Sepal.Width&quot;, &quot;Sepal.Length&quot;)] %&gt;% as.matrix %&gt;% scale y_all &lt;- (2 * (iris$Species == &quot;setosa&quot;)) - 1 x_all %&gt;% cbind(y_all) %&gt;% head ## Sepal.Width Sepal.Length y_all ## [1,] 1.01560199 -0.8976739 1 ## [2,] -0.13153881 -1.1392005 1 ## [3,] 0.32731751 -1.3807271 1 ## [4,] 0.09788935 -1.5014904 1 ## [5,] 1.24503015 -1.0184372 1 ## [6,] 1.93331463 -0.5353840 1 Er rest nu alleen nog het algoritme te trainen. Hiervoor gaan we de invoer data randomiseren in een training- en een test-set in de verhouding \\(3:1\\). from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split( r.x_all, r.y_all, test_size = 0.25, random_state = 42) De eigenlijke training gebeurt als volgt: model = Perceptron(learning_rate = 0.001) model.fit(x_train, y_train) Laten we eens kijken hoe het aantal mis-classificaties evolueerde over de verschillende cycli (deze cycli worden in ANN-wereld epochs genoemd): py$model$errs %&gt;% plot(type = &quot;b&quot;, pch = 19, cex = .8, ylab = &quot;# fouten per epoch&quot;, xlab = &quot;Epoch&quot;, main = &quot;Evolutie misclassificaties&quot;) Nu kunnen we de scheidingslijn in beeld brengen. Eerst maken we een raster. Voor elke punt in dit raster zullen we een voorspelling maken: x1_lim &lt;- x_all[, 1] %&gt;% range x2_lim &lt;- x_all[, 2] %&gt;% range grid_x &lt;- seq(x1_lim[1], x1_lim[2], l = 100) grid_y &lt;- seq(x2_lim[1], x2_lim[2], l = 100) grid &lt;- expand.grid(x = grid_x, y = grid_y) %&gt;% as.matrix grid_z &lt;- py$model$t(py$model$g(grid)) Nu plotten we de (gestandaardiseerd) data met de scheidingslijn (eng: boundary line) en duiden we de gebieden aan die, volgens het model, tot Iris setosa behoort of niet: plot(x_all[, 2] ~ x_all[, 1], pch = 19, cex = .8, col = 1 + (iris$Species == &quot;setosa&quot;), xlab = &quot;Kelkblad breedte (gestand.)&quot;, ylab = &quot;Kelkblad lengte (gestand.)&quot;, main = &quot;Setosa vs {versicolor, virginica}&quot;) curve((py$model$theta[1] * x + py$model$theta_0) / -py$model$theta[2], add = TRUE) points(grid[, 1], grid[, 2], pch = &quot;.&quot;, col = 1 + (1 + grid_z) / 2) legend(&quot;topright&quot;, c(&quot;{versicolor, virginica}&quot;, &quot;setosa&quot;), col = 1:2, pch = 19) Merk nog eens op dat het resultaat van ML inderdaad een functie is. In dit geval moet je de twee functies \\(f\\) en \\(t\\) achtereenvolgens uitvoeren. In de praktijk worden beide meestal verpakt in een functie predict, maar hier is ervoor gekozen om de functies gescheiden te houden, kwestie om de relatie te behouden met de Formules (6.1) en (6.2). 6.8 Voorspellen van de iris soort Laten we nu het model gebruiken om een voorspelling te maken op ongeziene data. from sklearn.metrics import accuracy_score print(accuracy_score(model.t(model.g(x_test)), y_test)) ## 1.0 Een perfect resultaat! Bronvermelding "],
["inleiding-tot-artificiële-neurale-netwerken.html", "Hoofdstuk 7 Inleiding tot Artificiële Neurale Netwerken 7.1 Geschakelde perceptronen 7.2 Feed-forward ANNs (FF-ANNs) 7.3 Types neuronen 7.4 Backpropagation 7.5 De verliesfunctie en kost-functies 7.6 Gradiënt afdaling 7.7 Stochastische en Minibatch gradiënt afdaling 7.8 Regularisatie", " Hoofdstuk 7 Inleiding tot Artificiële Neurale Netwerken Leerdoel 7.1 Begrijpt de fundamenten achter deep learning (EA_LD753). 7.1 Geschakelde perceptronen Een artificieel neuraal netwerk (ANN) kan gezien worden als een samenstelling van individuele perceptronen zodat de uitvoer van de ene perceptron de invoer van een andere wordt. Maar de neuronen worden niet kris-kras door elkaar geplaatst, maar worden georganiseerd in zogenaamde lagen (eng: layers). In een feed-forward netwerk resulteert dit in een gerichte acyclische graaf (eng: directed acyclic graph) van noden of neuronen. Figuur 7.1: Een fictief ANN met vier lagen. Het netwerk is een acyclische directionele graaf die van links naar rechts loopt. De meest linkse laag is de invoer-laag, de meest rechtse de uitvoer-laag. Één perceptron binnen het ANN is aangeduid in kleur en bestaat uit meerdere inkomende connecties, een node en één uitgaande connectie (hier niet getoond). We gaan later leren dat er ook nog andere manieren zijn om een ANN te visualiseren. Laten we een eenvoudig voorbeeld bekijken. In het hoofdstuk rond de perceptron konden we Iris setosa bloemen onderscheiden van andere soorten op basis van twee eigenschappen, namelijk de lengte en breedte van de kelkbladeren. Als we nu eens proberen de drie soorten te onderscheiden. Met één perceptron lukt het niet, want herinner je dat een perceptron enkel lineair gescheiden punten-wolken kan onderscheiden en een lijn zal telkens maar leiden tot een uitkomst met twee mogelijke categorieën. Doordat we nu meerdere uitkomsten hebben moeten we voor elke uitkomst een node (neuron, perceptron) voorzien. De invoer van deze 3 uitkomst-noden (“neuronen in de uitvoer laag”, eng: output-layer) is verbonden met de uitvoer van de 2 invoer-laag neuronen (omdat er twee eigenschappen zijn). Zo ziet het nieuwe netwerk er uit: library(neuralnet) nn = neuralnet(Species ~ Sepal.Length + Sepal.Width, data = iris, hidden = 0, linear.output = TRUE) nn %&gt;% plot(rep = &quot;best&quot;) Figuur 7.2: Een ANN om op basis van twee eigenschappen van de kelkbladeren van iris-bloemen drie soorten (setosa, vericolor en virginica) te onderscheiden. Dit netwerk heeft enkel een invoer- en een uitvoer-laag. De drie connecties naar de setosa-node plus deze node zelf komen overeen met de perceptron casestudy uit vorig hoofdstuk. De ‘Error’ geeft de totale fout weer. Verder in de tekst wordt deze aangeduid met het symbool \\(\\varepsilon\\) (epsilon). ‘Steps’ het aantal doorlopen epochs. Uit het bovenstaand netwerk kunnen we het volgende aflezen: Het bestaat uit twee lagen, een invoer-laag (eng: input layer) en een uitvoer-laag (eng: output layer) Het leeralgoritme heeft 2251 epochs nodig gehad om tot een stabiele oplossing te komen De totale fout die gemaakt wordt, is recht evenredig met \\(\\approx 26.4\\,mm^2\\) Of een bloem van de soort Iris setosa is heeft te maken met zowel de lengte als de breedte van het kelkblad, zij het in omgekeerde verhouding Vooral de breedte van het kelkblad bepaald of de bloem een versicolor is terwijl het eerder de lengte is dat bepaald of het een virginica is. De resultaten van bovenstaand netwerk kunnen als matrix \\(\\mathbf{\\theta}\\) worden weergegeven. Laten we deze matrix voor de volledigheid eens bekijken: \\[\\theta=\\begin{bmatrix} 0.775&amp;1.791&amp;-1.561 \\\\ -0.374&amp;0.014&amp;0.360 \\\\ 0.571&amp;-0.504&amp;-0.068 \\\\ \\end{bmatrix}\\] De totale fout waarvan hierboven sprake is, is evenredig met de som van de kwadraten van de deviaties tussen \\(y\\) en \\(\\hat{y}\\) en wordt berekend met de kost-functie \\(c\\) (eng: cost function): \\[\\begin{equation} \\varepsilon=c(y,\\hat{y})=\\frac{1}{2}\\sum_{n=1}^{N}{(y_n-\\hat{y}_n)^2} \\tag{7.1} \\end{equation}\\] Behalve een invoer-laag en een uitvoer-laag, kan een netwerk ook worden opgebouwd uit tussenliggende verborgen lagen (eng: hidden layers). Figuur 7.3 laat zien wat er gebeurt indien we aan het Iris-netwerk een verborgen laag toevoegen met daarin 3 noden. library(neuralnet) nn = neuralnet(Species ~ Sepal.Length + Sepal.Width, data = iris, hidden = 3, linear.output = TRUE) nn %&gt;% plot(rep = &quot;best&quot;) Figuur 7.3: Idem als voor Figuur 7.2, maar nu met toevoeging van een verborgen laag met 3 neuronen. Merk op dat de totale fout \\(\\varepsilon\\) nu kleiner is geworden, maar dat betekent nog niet dat dit een goed idee is om de extra verborgen laag inderdaad toe te voegen. Oefening 7.1 (Méér verborgen lagen) Het toevoegen van een extra verborgen laag in ons model zorgt ervoor dat de totale fout \\(\\varepsilon\\) op de Iris dataset verkleint. Maar dat betekent nog niet dat het een goed idee is om een laag toe te voegen. Waarom niet? Leg uit in eigen woorden. 7.2 Feed-forward ANNs (FF-ANNs) We gaan nu iets specifieker moeten zijn in wat de regels rond een ANN zijn en hoe een ANN aan zijn oplossing komt. Om te beginnen leggen we de regels vast voor de noden en connectoren (pijlen) een FF-ANN: Stelling 7.1 Noden: De invoer-laag is verplicht en bevat één node voor elke feature De uitvoer-laag is ook verplicht en bevat één node voor elke afhankelijke variabele (regressie) of één node voorelke categorie van de uitkomst (classificatie)2 Verborgen lagen zijn optioneel Het aantal noden in de verborgen lagen is bij realistische situaties met vele features meestal veel kleiner dan het aantal features zodat de invoer a.h.w. gecomprimeerd wordt Elke laag, behalve de uitvoer-laag, kan een speciale afwijking-node (eng: bias-node) bevatten met de constante waarde 1 Stelling 7.2 Connectoren: In een FF-ANN mogen connectoren niet twee noden van dezelfde laag verbinden Bovendien moet de zin van de connectoren lopen van invoer-laag naar uitvoerlaag en nooit andersom Het is niet zo dat alle noden van de ene laag moeten verbonden zijn met alle noden van de volgende laag Hoe werkt een FF-ANN nu eigenlijk? Wel, als je het perceptron begrijpt is er eigenlijk niet veel aan. De waarden van de noden binnen de invoer-laag zijn uiteraard gekend. Voor elke andere laag in het netwerk bekom je de nieuwe waarden door de waarden uit de voorgaande laag te vermenigvuldigen (via inwendig product, zie Appendix) met de bijhorende gewichten: \\[\\begin{equation} z_\\Lambda=g(x_\\Lambda)=\\mathbf{x_\\Lambda}\\cdot\\mathbf{\\theta_\\Lambda} \\\\ \\hat{y}_\\Lambda=t(z_\\Lambda) \\\\ \\Lambda\\subset\\{2, 3,..,\\mathscr{L}\\} \\tag{7.2} \\end{equation}\\] , waarbij \\(\\Lambda\\) de index van de laag moet voorstellen. Het leeralgoritme begint met willekeurige gewichten en neemt de eerste set van instanties (de zogenaamde minibatch) en vult de waarden in voor de noden van de invoer-laag. Daarna zal het leeralgoritme de inwendige producten berekenen en zo de waarden voor de volgende lagen binnen het netwerk invullen. Komt het FF-ANN leeralgoritme aan de laatste laag van het netwerk aan, dan wordt de geschatte uitkomst \\(\\hat{y}\\) vergeleken met de werkelijke uitkomst \\(y\\). Op basis hiervan zal het leeralgoritme de gewichten aanpassen. 7.3 Types neuronen Tot hier toe hebben we uitsluitend lineaire neuronen besproken, maar om complexere problemen te kunnen afhandelen was er een manier nodig om een niet-lineaire respons in de neuronen in te bouwen. Dit doen we door van de transformatie functie \\(t(z)\\) een niet-lineaire functie te maken. Het type transformatie-functie bepaald dan ook het type neuron. Er zijn vele types lineaire en niet-lineaire neuronen, maar enkel de meest courante worden hieronder visueel weergegeven. Figuur 7.4: Vier neuron-types gebaseerd op hun transformatie- functies \\(t(z)\\). De functie voor de sigmoïd is \\(\\frac{1}{1+e^{-z}}\\), die van het tanh neuron type is uiteraard \\(tanh(x)\\) en die voor het Restricted Linear Unit neuron (ReLU) is \\(max(0, z)\\). Sommige restricties gelden voor alle neuronen binnen een laag. Zo is er de zogenaamde softmax-type uitvoer-laag. Hierbij stellen de neuronen binnen die laag een kansverdeling voor en moet bijgevolg de som van de neuronen op exact 1 uitkomen: \\[\\begin{equation} \\hat{y}_\\Lambda=\\frac{e^{z_\\Lambda}}{\\sum_{\\Lambda}{e^{z_\\Lambda}}} \\tag{7.3} \\end{equation}\\] 7.4 Backpropagation Hierboven stond er: Op basis hiervan zal het leeralgoritme de gewichten aanpassen. Tijdens het trainen van meerlagige ANNs maken we gebruik van het backpropagation algoritme (zie Rumelhart et al. (1986)) een voorbeeld van dynamisch programmeren (eng: dynamic programming). Hieronder zijn de stappen uiteengezet: Initialisatie: De architectuur van het netwerk en de beginwaarden van de parameters worden toegekend Invoer: Ingeven van de waarden van de invoer laag Forward pass: De lagen één voor één doorlopen, te beginnen bij de eerste laag na de invoerlaag en bij elke laag berekenen we \\(x_{\\Lambda+1}=t_\\Lambda(g(x_\\Lambda)), \\Lambda\\subset\\{1,..,\\mathscr{L}-1\\}\\) Bereken de kost: Met de waarden in de uitvoerlaag, berekenen we de restterm \\(\\varepsilon\\). Backpropagation: Nu berekenen we voor elke laag, van laatste naar tweede, de afgeleiden \\(\\frac{\\partial{E_n}}{\\partial\\theta_{n-1}}\\) (eng: error derivatives) die aangeven hoeveel de restterm van een laag verandert naarmate de parameters van de neuronen uit de vorige laag veranderen Wijzigen parameters: We vullen nu de leersnelheid in en lossen de zogenaamde modificatie-formule (eng: modification formula) op om de nieuwe parameter waarden (= gewichten) te berekenen en we kunnen voort naar de volgende epoch Figuur 7.5: Schematische weergave van het backpropagation algoritme uitgewerkt in een begeleid ML context. Links staat de dataset met de onafhankelijke variabelen, rechts de uitkomsten. Centraal de weergave van een ANN waarvan de dikte van de connectoren overeenstemmen met de waarden voor \\(\\theta\\). Zie de blog (link lokale kopij met aantekeningen) van Jay Prakash voor een uitgewerkt voorbeeld en de precies berekening van de afgeleiden (Prakash (2017)). 7.5 De verliesfunctie en kost-functies Met een verliesfunctie (\\(v\\); eng: loss function) bereken je, binnen een epoch, de prestatie van een model door de uitvoer voor die epoch het model \\(\\hat{y}\\) te vergelijken met de werkelijke uitkomst \\(y\\). Heb je meerdere epochs doorlopen, dan kan je de voorspellingen van de ganse training set vergelijken met de reële waarden. Stelling 7.3 (Verliesfunctie vs kost-functie) Een verliesfunctie werkt binnen een epoch op een enkele instantie of op een minibatch. Een kost-functie is een soort beoordelingsfunctie (eng: objective function) die werkt op een volledige episode waarbij de ganse training set in rekening wordt gebracht en waarbij vaak ook regularisatietermen beschouwd worden (zie later). Er zijn veel mogelijke verschillende verlies- en kost-functies afhankelijk van het beoogde doel en type van de uitkomst-variabele. Een populaire keuze wanneer het gaat om reële getallen is de som van de kwadraten van de verschillen (eng: sum of squared errors of kort SSE) of het gemiddelde daarvan (eng: mean sum of squares of MSE). Bij SSE en ANN gebruikt men meestal over de helft van de som van de kwadraten omdat deze vorm differentieerbaarder is. \\[\\begin{equation} \\varepsilon=c(y,\\hat{y})=\\frac{1}{2}\\sum_{n=1}^{N}{(y_n-\\hat{y}_n)^2} \\tag{7.4} \\end{equation}\\] Naarmate dat het voorspelde resultaat \\(\\hat{y}\\) steeds meer begint af te wijken van het werkelijk resultaat \\(y\\), neemt de restterm \\(\\varepsilon\\) razendsnel toe (zie interactieve figuur hieronder) en het doel van het leeralgoritme is om de parameters \\(\\mathbf{\\theta}\\) zodanig aan te passen dat de fout net zijn klein mogelijk wordt. Men zegt dat het doel is om de verlies- en kost-functies te minimaliseren (eng: minimize). Een andere belangrijke verliesfunctie is de cross-entropie: Deze wordt gebruikt wanneer de uitkomst variabele een kansverdeling (eng: probability distribution) voorstelt, i.e. wanneer de waarden voor de uitkomsten reële getallen getallen zijn in het bereik \\([0, 1]\\) en wanneer alle waarden voor de categorieën in de uitvoerlaag van het netwerk optellen tot 1. Deze laatste wordt gewaarborgd door de zogenaamde softmax restrictie (zie Types neuronen). \\[\\begin{equation} c(y, \\hat{y})=-\\frac1N\\sum_{n=1}^{N}{\\left[y_n\\,log(\\hat{y}_n) + (1-y) \\,log(1-\\hat{y})\\right]} \\tag{7.5} \\end{equation}\\] Het is mooi om zulke verliesfunctie in beeld te brengen, maar in werkelijkheid spelen er meer dan twee parameters in het spel en zijn niet alle neuronen lineair. Als gevolg zal de verliesfunctie de vorm van een hypervlak in een meer-dimensionale ruimte gaan innemen en zal de vorm ook veel grilliger zijn dan hier voorgesteld. 7.6 Gradiënt afdaling De bedoeling is om het laagste punt van de hierboven gedemonstreerde verliesfunctie te vinden. Indien het werkelijk om kwadratische verlies-oppervlakten zou gaan, dan was er geen probleem omdat met het laagste punt dan analytisch zou kunnen vinden, i.e. met een relatief eenvoudige matrix bewerking. Maar in werkelijkheid zijn de verliesfuncties grillig en meerdimensionaal (7.7). Dan is er een soort trial-and-error manier nodig om het laagste punt te vinden. Figuur 7.6: Onderscheid tussen kwadratische, convexe en grillige functies, hier voor de eenvoud voorgesteld als abstracte twee-dimensionale functies. Het zoeken naar zulk een minimum van zulk een functie noemt men een optimalisatie-probleem en een groot deel van het onderzoek naar ANNs situeert zich rond het vinden van een efficiënt optimalisatie algoritme. Deze optimalisatie is trouwens waarnaar in de paragraaf rond de perceptron verwezen werd als zijnde de administratie rondom de functies \\(g\\) en \\(t\\). Figuur 7.7: Voorbeeld van een realistische geprojecteerde verlies-functie bekomen door Ankur Mohan volgens de methode beschreven in Li et al. (2018). Kijk ook eens op losslandscape.com voor artistieke rendering van ‘verlies-landschappen’. Maar goed, laten we, om de eenvoud te bewaren, toch nog even voortbouwen op een verliesfuncties met een eenvoudige convexe vorm. Bij het initialiseren van de parameters met willekeurige waarden stellen we ons een willekeurig punt voor in het \\(\\theta_1\\perp\\theta_2\\)-vlak. Vanuit dit punt projecteren we ons op het verlies-oppervlakte, door parallel te lopen met de \\(\\varepsilon\\)-as totdat we het oppervlakte kruisen. We bevinden ons nu ergens op dit oppervlak, in het punt \\(e\\). Hoe vinden we nu het laagste punt op dit oppervlak? Simpel, volg de weg die het steilst naar beneden gaat en, in het geval van een convexe functie, kom je vanzelf uit op het laagste punt. De weg van de steilste afdaling (eng: steepest ascent) wordt wiskundig uitgedrukt als de gradient \\(\\nabla\\varepsilon\\) in \\(e\\). Een gradient kan gezien worden als een meerdimensionale afgeleide en kan in een 2-dimensionale context (2 parameters) voorgesteld worden als een raakvlak in het punt \\(e\\). Vandaar de naam van een familie aan optimalisatie-algoritmen en ook de titel van deze paragraaf: gradiënt afdaling (eng: gradient descent). We weten nu in welke richting we moeten gaan, het enige dat overblijft is de grootte van de stap die we in die richting nemen. De grootte van de stap wordt bepaald door de leersnelheid \\(r\\) (eng: learning rate). Nemen we te grootte stappen, dan kunnen we het minimum missen doordat we er over springen. Nemen we te kleine stappen, dan duurt het mogelijk te lang en riskeren we bovendien om achter kleine heuveltjes van de ruis te komen vastzitten. Wanneer we voldoende dicht komen bij het (lokaal) minimum van het verlies-oppervlakte, verwachten we dat de gradient in dat punt heel erg klein wordt. We zeggen dan dat het optimalisatie-algoritme convergeert. Wat ‘voldoende dicht’ precies moet zijn, wordt door de datawetenschapper bepaald en is meestal een arbitrair klein getal. Blijft tijdens het zoeken de gradient fluctueren, dan kan de datawetenschapper beslissen dat de zoektocht vroegtijdig gestaakt wordt alvorens er convergentie bereikt wordt. Er is immers niet altijd een garantie dat convergentie mogelijk is en zelfs als je optimalisatie-algoritme convergeert, heb je bij grillige oppervlakten nooit de garantie dat het lokaal minimum ook het globaal minimum is. Stelling 7.4 Bij complexe en grillige verlies-oppervlakten zal het vakk zo zijn dat er geen garanties gegeven kunnen worden dat het optimilsatie-algoritme convergeert of dat het gevonden minimum ook de best mogelijk oplossing is. Wiskundig kunnen we de gradiënt-afdaling van het optimalisatie-algoritme als volgt samenvatten: \\[\\begin{equation} \\theta\\leftarrow\\theta-r\\cdot\\nabla\\varepsilon(\\theta) \\tag{7.6} \\end{equation}\\] De leersnelheid wordt vaak, zoals bij de familie aan optimalisatie-algoritmes die men simluated annealing noemt, bij elke epoch aangepast aan de zogenaamde temperatuur die langzaam afneemt. 7.7 Stochastische en Minibatch gradiënt afdaling De afdaling zoals beschreven in vorige paragraaf baseert zich op verlies-landschappen die berekend werden op de ganse dataset. In dat geval spreekt men van batch gradiënt afdaling (eng: batch gradiënt descent). Zoals reeds aangehaald, deze strategie werkt niet goed wanneer het oppervlakte van het verlies-landschap grillig is zoals in het voorbeeld hier direct boven. Die grilligheid wordt veroorzaakt omdat net de verliesfuncties van alle instanties in de dataset samen worden beschouwd. In de stochastische gradiënt-afdaling (eng:stochastic gradient descent) worden de verlies-hypervlakken berekend telkens voor slechts één instantie tegelijk. Deze benadering maakt dat de rekentijd bij grote datasets te hoog zou oplopen. Daarom is er een tussenvorm bedacht, de minibatch gradiënt-afdaling (eng: minibatch gradient descent). HIer wordt telkens een subset (de minibatch) van de dataset gebruikt om het verlies-oppervlak te berekenen. De minibatch grootte is een hyperparameter van het leeralgoritme. Je kan waarschijnlijk hele bibliotheken vullen met informatie over de verschillende optimalisatie-algoritmen, maar hier volstaat het om te weten dat er erg veel verschillende zijn en dat er nog actief onderzoek naar wordt gedaan. 7.8 Regularisatie Regularisatie (eng: regularization) is een manier om het overfitten tegen te gaan. Het wordt algemeen gebruikt in ML en is niet bepaald specifiek voor neurale netwerken. Het is een heel eenvoudig principe. Het komt erop neer dat er aan de kost-functie een term wordt toegevoegd dat ervoor zorgt té complexe modellen worden afgestraft. Specifiek voor ANNs gaat het om een term die te grote gewichten \\(\\mathbf{\\theta}\\) afstraft. \\[\\begin{equation} \\varepsilon&#39;=\\varepsilon+\\lambda \\cdot f(\\theta) \\tag{7.7} \\end{equation}\\] In de formule (7.7) dient \\(f(\\theta)\\) om de vorm aan te geven van de functie die toeneemt naarmate \\(||\\theta||\\) toeneemt en is \\(\\lambda\\) een hyperparameter die de sterkte aangeeft waarmee te grote parameters moeten worden afgestraft. Met \\(\\lambda=0\\) nemen we geen maatregelen tegen overfitting met alle gevolgen van dien. Een te grote \\(\\lambda\\)-waarde is ook nefast omdat het leerproces dan te fel gehinderd wordt. Er zijn meerdere regularisatie termen mogelijk. Een gangbare regularisatie is de zogenaamde L2 regularisatie. Dit is dus een regularisatie op basis van L2, de euclidische norm (Formule (7.8)). \\[\\begin{equation} f(\\theta)=||\\mathbf{\\theta}||^2=\\sqrt{\\theta_1 ^ 2+\\theta_2 ^ 2+..+\\theta_n ^ 2} \\tag{7.8} \\end{equation}\\] Omdat echter the vierkantswortel een vorm van schaling veroorzaakt en omdat \\(\\lambda\\) deze functie al overneemt gebruikt men meestal de volgende regularisatie-functie: \\[\\begin{equation} f(\\theta)=\\frac12\\lambda\\sum{\\theta^2} \\tag{7.9} \\end{equation}\\] Bij elke epoch verkleinen de waarden van de parameters lineair naar nul. Daarom wordt er naar deze regularisatie verwezen met de term weight decay. Het gevolg is dat alle parameters a.h.w. de kans krijgen om een bijdrage te leveren in het maken van de voorspellingen. Een andere regularisatie term is de L1 norm: \\[\\begin{equation} f(\\theta)=\\lambda\\sum{|\\theta|} \\tag{7.10} \\end{equation}\\] Deze regularisatie werkt dus in op de absolute waarde van de parameters. Als gevolg zullen er vele parameters tijdens de optimalisatie quasi nul worden (vergeet niet dat computers beperkt zijn in het weergeven van erg kleine getallen). Bijgevolg worden de parameters eruit gefilterd die onvoldoende bijdragen aan de voorspelling. Omdat men in de praktijk de voordelen van zowel de L1 als de L2 wil benutten bestaat er een tussenvorm: elastisch net regularisatie (eng: elastic net regularization). Beide termen worden in dit geval aan de beoordelingsfunctie toegevoegd, de ene met een factor \\(\\alpha \\subset [0, 1]\\), de andere met een factor \\(1-\\alpha\\), alweer een hyperparameter. Nog een regularisatie is de max norm beperking (eng: max norm constraint). Hier wordt er een plafond ingesteld op de magnitude van de inkomende parameter vector van een neuron. Wordt \\(||\\theta||_2 &gt; c\\) dan wordt de co-vector verkleind met de hoeveelheid dat het groter dan \\(c\\) was. De laatste vorm van regularisatie die hier besproken wordt, noemt men dropout (nl: uitval of uitvaller). Dit is een erg populaire regularisatie. Tijdens het trainen mogen neuronen actief blijven met een bepaalde waarschijnlijkheid \\(p_{act}\\). Bij elke epoch is er dus als het ware een loterij. Elk neuron trekt een lotje \\(p_k\\). Als de waarde van \\(p_k&lt;P_{act}\\), dan mag het neuron niet meespelen in de volgende ronde, anders wel. Op deze manier voorkomt men dat een neuraal netwerk té afhankelijk wordt gemaakt van bepaalde input Figuur 7.8: Het proces van dropout regularisatie. De getallen zijn de probabiliteiten die het gevolg zijn van de loterij waarvan sprake is in de tekst. Aangepast van figuur 2-16 van Buduma and Locascio (2017). Om het testen mogelijk te maken tijdens zulke regularisatie, wordt de uitvoer van neuronen die niet of non-actief werden geplaatst gedeeld met de waarde \\(p_{act}\\). Deze aanpassing noemt men omgekeerde uitval (eng: inverted dropout). Bronvermelding "],
["convolutionele-neurale-netwerken-cnn.html", "Hoofdstuk 8 Convolutionele Neurale Netwerken (CNN) 8.1 Het onstaan van Machine Vision 8.2 Waarom vanilla SGD netwerken ontoereikend zijn 8.3 De CNN Filter 8.4 De filter binnen een NN 8.5 Filter als compressor 8.6 De stride niet opgeven 8.7 De volledige filter-laag 8.8 Meerdere filters per laag 8.9 Max Pooling 8.10 Samenstellen van CNNs", " Hoofdstuk 8 Convolutionele Neurale Netwerken (CNN) 8.1 Het onstaan van Machine Vision Computer vision is een oude discipline die zich bezighoudt met het automatisch interpreteren van onder andere stilstaande beelden en videobeelden. Oorspronkelijk was het doel van deze discipline om digitale beelden te bewerken (eng: image processing) zodat er kenmerken (eng: features) geëxtraheerd konden worden. Een voorbeeld is het ontwikkelen van algoritmen voor het ontdekken van contrasterende randen (eng: edge detection). Voor gezichtsherkenning, bijvoorbeeld, was het idee dat je de ogen van een gezicht kan herkennen door twee zwarte gebieden met daartussen een lichter gebied (van de neus). Hoewel dat inderdaad een logische benadering lijkt te zijn, blijkt dit in de praktijk toch ontoereikend (zie Figuur 8.1). Figuur 8.1: Negen gezichten. Men kan veronderstellen dat ogen altijd donkerder zijn dan de neus en dus een gemakkelijke manier om het kenmerk ‘ogen’ te onderscheiden. Hier wordt dit idee getest. Met het minimalistisch software-pakket IrfanView werd er ter hoogte van de ogen een selectie van 35 × 434 pixels gemaakt, omgezet naar grijswaarde en getransformeerd met het pixelize-algoritme met parameter 19. Bij een aantal gevallen zie je inderdaad het patroon donker-licht-licht-donker. Aan de meeste andere gezichten, misschien ten gevolge van het dragen van een bril of het bezitten van een andere huidskleur, zie je meteen dat deze procedure ontoereikend is. Deze gezichten werden gegenereerd door het algoritme van Tero Karras Karras et al. (2020). Voor alle duidelijkheid: het gaat hier dus niet om bestaande mensen. Ondertussen behoort computer vision bijna volledig toe aan de discipline van ML. Inderdaad, met de opkomst van neurale netwerken was er een manier gevonden om de feature selectie-procedure volledig te automatiseren. Een van de belangrijkste vooruitgang in de ontwikkeling van convolutionele NN werd verwezenlijkt door Alex Krizhevsky Krizhevsky et al. (2012). De details van zijn ontdekkingen gaan we in de volgende paragrafen bespreken, maar het is interessant te weten dat de ontdekkingen er gekomen zijn door de natuur na te bootsen, meer bepaald het gezichtsvermogen bij zoogdieren (zie Figuur 8.2). Persoonlijkheid 8.1 (Alex Krizhevsky) Alex Krizhevsky won op spectaculaire wijze de ImageNet competitie van 2012 door een algoritme te ontwikkelen dat de bestaande accuraatheid van 73.9% (en waar elke honderste van een percent nog het verschil maakte) plots overtrof met een accuraatheid van 84%. Figuur 8.2: Spraakmakend experiment waarbij werd aangetoond dat sommige neuronen in de visuele cortex van huiskatten (maar dit geldt als model voor ander zoogdieren, waaronder de mens) enkel afgevuurd worden bij het zien van verticale lijnen Hubel and Wiesel (1959). 8.2 Waarom vanilla SGD netwerken ontoereikend zijn Waarom zouden er voor beeldherkenning nieuwe algoritmen ontwikkeld moeten worden. We zagen eerder in deze cursus dat de cijfers van de MNIST dataset herkend kunnen worden door gebruik te maken van een standaard NN met standaard optimalisatie algoritmen (eng: vanilla SGD, vanilla stochastic gradient descent). Het probleem is de schaalbaarheid. De afbeeldingen van de MNIST dataset bestonden uit 28 pixels × 28 pixels in grijswaarden. De invoerlaag wordt dan al meteen 784 noden groot en het model bestond uit méér dan 100 000 parameters. Bedenk dan hoe groot het netwerk zou moeten worden voor de analyse van 4K video met 3840×2160 pixels aan 60 beelden per seconde. Het komt erop neer dat de volledige geconnecteerde lagen (eng: fully-connected layers) snel té zwaar worden voor onze huidige computers, een gevaar vormen voor overfitting en algemeen elektron-verspillend zijn. Het aantal parameters moet dus drastisch verlagen om het gebruik van NN voor real-time beeldherkenning mogelijk te maken. Een dropout regularisatie biedt hier onvoldoende soelaas omdat je hiermee te veel informatie negeert en belangrijke patronen mist die zich op kleine schaal manifesteren. 8.3 De CNN Filter Een van de elementen die convolutionele NN mogelijk maakte was het gebruik van filters. Een filter is een patroon waarmee een input wordt gescand. Laten we Figuur 8.3 grondig onderzoeken. Links wordt een eenvoudige afbeelding voorgesteld met afmetingen 8 pixels × 8 pixels × {0, 1}. Filter 1 stelt een verticale patroon voor, Filter 2 een horizontaal patroon. Tijdens de convolutie-operatie (voorgesteld met de symbolen ∗ of, indien het om circulaire data gaat, \\(\\circledast\\)) worden de filters a.h.w. over de afbeelding geschoven en telkens gecontroleerd in hoeverre de afbeelding op die locatie het patroon vertoont. In deze binaire filter is het resultaat van de convolutie een 1 telkens wanneer het patroon exact overeenkomt en een 0 als het niet perfect overeenkomt. Figuur 8.3: De demonstratie van een CNN filter. In dit vereenvoudigd voorbeeld gaat het om een ‘binaire’ 2D filter in de zin dat het resultaat van een scan-operatie binaire is en dat de invoer 2-dimensionaal is. Zie tekst voor details. Deze figuur werd aangepast van figuur 5-6 uit Buduma and Locascio (2017). In onderstaande video zien we de filter in actie. Merk op dat het resultaat van deze filter al iets complexer is zoals voorgesteld met de groene (meer positieve match) en rode kleuren (meer negatieve match; i.e. omgekeerd patroon). Na het toepassen van de filter wordt er een ReLU activatie functie toegepast en later een Pooling (zie later). Dit is fragment uit de presentatie van Otavio Good tijdens de TheAIConf in 2017 8.4 De filter binnen een NN Hoe vertaalt dit zich nu naar een neuraal netwerk? In een NN stemt de filter operatie overeen met het meervoudig uitvoeren van inwendige producten tussen invoerlaag en filter-matrix. Het is belangrijk te begrijpen dat de filter gemeenschappelijk is voor de ganse filter-laag. Dat wilt zeggen dat elk neuron van de invoerlaag met dezelfde filter wordt vermenigvuldigd. De convolutie in dit verhaal vertaalt zich als een soort herhalingslus.3 Figuur 8.4: In een NN stemt de filter operatie overeen met het meervoudig uitvoeren van inwendige producten tussen invoerlaag en filter-matrix. 8.5 Filter als compressor Omdat binnen een laag dezelfde filter wordt gebruikt voor alle inkomende connecties, beperkt men zeer sterk het aantal parameters: Figuur 8.5: Een enkele filter heeft slechts een beperkt aantal parameters nodig. In werkelijkheid zal het iets meer dan 3 zijn, in dit voorbeeld, zie later in de tekst. Natuurlijk komt men niet toe met slechts één filter. Het is niet voldoende om enkel horizontale of verticale patronen te herkennen, misschien willen we ook krommen, schuine lijnen en dergelijke…. Het aantal filters neemt dan wel toe, maar niet langer exponentieel met toenemende grootte van de afbeeldingen. 8.6 De stride niet opgeven Er zijn (hyper)parameters die een filter kenmerken. De eerste die we hier bespreken is de stride (nl: letterlijk de pas; stapgrootte). Het geeft eenvoudigweg aan met welke frequentie de filter operatie moet uitgevoerd worden. Dit is weer een andere manier waarmee een filter het aantal neuronen weet te beperken, maar opgelet. Een stride &gt; 1 betekent dat je onvermijdelijk patronen gaat missen. Figuur 8.6: De demonstratie van de stride hyperparameter van een filter. De tweede hyperparameter is de padding. Dit is de ‘dikte’ van de rand met nul-waarden die rond de ‘afbeelding’ van de input laag wordt aangebracht om ervoor te zorgen dat de filter operatie de dimensies niet wijzigt zoals het geval is in Figuur 8.4. 8.7 De volledige filter-laag De filter-operatie vervangt de traditionele matrix-operatie die eerder aangeduid werd met functie \\(g\\). De output is echter niet een getal maar een zogenaamde feature map. Dat betekent dat er na de filter operatie nog steeds een activatiefunctie volgt. Dat is vaak een ReLU functie omdat deze de meest eenvoudige niet-lineaire functie is. 8.8 Meerdere filters per laag Als we meerdere filters kunnen hebben en elke filter genereert een feature map met gelijkaardige dimensies als de vorige laag in het netwerk, dan betekent dit dat de feature maps dus een extra dimensie hebben, i.e. in plaats van lagen (matrices), krijgen we blokken (3-dimensionele arrays). Bovendien, de filter die hierboven werd voorgesteld, was twee-dimensionaal. Dat is voldoende voor afbeeldingen in grijswaarden zoals de MNIST dataset. Werk je echter met RGB waarden, dan kan je iets bijzonder doen. Je kan een 3D filter definiëren van bijv. 4 pixels × 4 pixels × 3 kleurwaarden en de convolutie doorvoeren als voorheen, alleen nu in de derde dimensie. Figuur 8.7: 2D-convolutie met 3D filter. Bron: Kunlun Bai (2019). We kunnen nu 2D beelden vervangen door 3D beelden (zoals MRI scans of dergelijke) of videobeelden (tijd is dan de derde dimensie) en dan kan de convolutie plaatsvinden 3-dimensies in plaats van 2, men spreekt dan ook van 3D convoluties. Figuur 8.8: 3D-convolutie met 3D filter. Bron: Kunlun Bai (2019). 8.9 Max Pooling Hier kunnen we heel kort zijn, dit is letterlijk een soort beeld-compressie, behalve dan dat je telkens de maximum ‘pixel-waarde’ neemt i.p.v. een gemiddelde. Er bestaan dan ook anti-aliasing varianten. Figuur 8.9: Demonstratie van Max pooling. Deze operatie wordt, indien beschouwd als onderdeel van een laag, altijd toegevoegd ná de activatiefunctie, We krijgen dus (\\(m\\) = max pool): \\[x_{out} = m(t(g(x_{in})))\\] Vaak wordt deze echter als afzonderlijke laag beschouwd. 8.10 Samenstellen van CNNs We kunnen nu meerdere lagen samenbrengen in een heus netwerk. Als we begrijpen dat een filter-operatie de informatie probeert te capteren, dat een ReLU-operatie (of andere activatie-functie) de informatie combineert en de max pool-operatie de informatie comprimeert, dan zijn de volgende regels niet al te onlogisch: De filter + activatie-functie vormen samen een zogenaamde convolutie laag De max-pool laag is niet verplicht De max-pool laag kan afzonderlijk voorkomen, zonder voorafgaande convolutie De max-pool-laag zal vaak voorkomen na een convolutie laag De combinatie van convolutie-laag en max-pool-laag wordt vaak meerdere keren na elkaar herhaald om telkens partonen met toenemende complexiteit te capteren Nog wat links: Visualisatie van CNN lagen Interactief voorbeeld Verdiepeing in verschillende soort CNNs Bronvermelding "],
["de-logaritme.html", "A De Logaritme", " A De Logaritme De logaritme is een functie en die als het omgekeerde van een exponentiële functie beschouwd kan worden: \\[ y = ^g\\!log(x) \\\\ g^y=x \\] waarbij \\(g\\) het grondtal is. Indien het niet meegegeven wordt, veronderstelt men dat \\(g\\) gelijk is aan \\(e = 2.718281828..\\) en komt de logaritme overeen met de natuurlijk logaritme dat in België en sommige andere delen van de wereld aangeduid wordt met \\(ln(x)\\). Een belangrijke reden om een variabele te transformeren met de logaritme is omdat de verdeling ervan een grote scheefheid vertoond waardoor: de verdeling moeilijker te interpreteren is en de verdeling te sterk afwijkt van normaliteit, een assumptie waar erg veel statistieken van afhangen In het R pakket MASS vinden we een heel simpele dataset mammals met gemiddelde lichaamsgewichten en hersenen-massa’s. Deze data laat het probleem alsook het effect van de log-transformatie visueel zien: Figuur A.1: Lichaamsgewichten alleenstaand als verdelingen (boven) of tegenover hersenen-massa’s (onder) in de lineaire schaal (links) of de logaritmische schaal (links). De blauwe balken geven het gemiddelde (volle blauwe lijn) weer ± standaardafwijking (blauwe stippellijnen). Bedenk wel dat enkel strikt positieve lineair-numerieke meetwaarden (\\(\\mathbb{R}^+\\)) voor een log-transformatie in aanmerking komen. curve(log(x), xlim = c(-5, 10), col = dyad[16], lwd = 2) polygon(c(-7, -7, 0, 0), c(-3, 3, 3, -3), angle = 45, density = 10, col = dyad[16]) Op internet kan je trucjes vinden om log-transformaties te doen op data die 0 waarden bevatten. Een voorbeeld van zo een trucje is om bij elke waarde ½ bij op te tellen. Zulke procedures kunnen helpen voor de visualisatie maar hebben vaak geen statistische grondslag, dus opgelet hiermee. We staan er niet meer bij stil hoe fantastisch het eigenlijk is om in een machine log(2.34651) te kunnen opgeven. Wil je weten hoe men vroeger logs berekende? Net als voor andere functies (sin, tanh, …) gebeurde dit aan de hand van tabellen: Figuur A.2: Zo ging men vroeger te werk voor het berekenen van een logaritme! Oefening A.1 (Probeer het uit) Maak nu zelf eens een vector van negen waarden gaande van 1E-4 tot 1E4 en neem de logaritme van deze vector. Oefening A.2 (Anti-log) Deze paragraaf begon met de stelling dat de logaritme als het omgekeerde van een exponentiële functie kan beschouwd worden. Toon dat eens voor jezelf aan. Probeer eens zeer kleine en zeer grote waarden uit voor \\(x\\) en probeer eens verschillende grondtallen uit. "],
["inwendig-product-matrix-vermenigvuldiging-vectoren-en-tensoren.html", "B Inwendig product, matrix-vermenigvuldiging, vectoren en tensoren", " B Inwendig product, matrix-vermenigvuldiging, vectoren en tensoren De perceptron is een leeralgoritme dat de ‘gewichten’ \\(\\theta\\) probeert te vinden die na vermenigvuldiging met de invoer \\(x\\) de uitkomst \\(y\\) tracht te benaderen met \\(\\hat{y}\\). \\[\\begin{equation} f(x, \\theta)=\\sum_{i=0}^{n}{\\theta_ix_i} \\tag{B.1} \\end{equation}\\] Deze bewerking komt overeen met het inwendig product van de matrix \\(\\mathbf{x}\\) met de vector \\(\\mathbf{\\theta}\\): \\[\\begin{equation} \\mathbf{x}\\cdot\\mathbf{\\theta} \\tag{B.2} \\end{equation}\\] Nemen we iris dataset uit een vorige casestudy, dan ziet de invoer van een neuraal netwerk \\(\\mathbf{x}\\) er als volgt uit: x &lt;- iris[, 1:2] %&gt;% cbind(&quot;Bias&quot; = c(1, 1), .) %&gt;% as.matrix \\[\\mathbf{x}=\\begin{bmatrix} 1.0&amp;5.1&amp;3.5 \\\\ 1.0&amp;4.9&amp;3.0 \\\\ 1.0&amp;4.7&amp;3.2 \\\\ 1.0&amp;4.6&amp;3.1 \\\\ 1.0&amp;5.0&amp;3.6 \\\\ 1.0&amp;5.4&amp;3.9 \\\\ \\vdots&amp;\\vdots&amp;\\vdots \\\\ \\end{bmatrix}\\] De eerste kolom is de nep-variabele waarvan sprake zal zijn tijdens het bespreken van Feed-forward ANNs. Nemen we hier nu de eerste rij uit, dan krijgen we een geannoteerde [vector](https://nl.wikipedia.org/wiki/Vector_(wiskunde). In R doen we dit: x[1,] ## Bias Sepal.Length Sepal.Width ## 1.0 5.1 3.5 Wiskundigen gaan een vector echter meestal voorstellen als een kolomvector. Datawetenschappers houden er dan weer van om een vector als rijvector voor te stellen. Dat heeft te maken met het feit dat het aantal variabelen vaak veel kleiner is dan het aantal instanties en, zoals hierboven voor \\(\\mathbf{x}\\) gedaan werd, de matrix gemakkelijker weer te geven is. Hier gaan we de wiskundige weergave volgen, maar het resultaat moet hetzelfde zijn. We bekomen een kolomvector door deze eerste rij te transponeren, i.e. we maken van de rijen kolommen: \\[\\mathbf{x^T_1}=\\begin{bmatrix} 1.0 \\\\ 5.1 \\\\ 3.5 \\\\ \\end{bmatrix}\\] We kunnen deze vector interpreteren als een pijl in een 3-dimensionaal assenstelsel vertrekkende van de oorsprong en met de punt ter hoogte van 1 op de as Bias, 5.1 richting as Sepal.Length en 3.5 richting Sepal.Width: Maar opgelet: Stelling B.1 Het typeert een vector dat het uit waarden bestaat die elke op hun eigen dimensie in een \\(p\\)-dimensionale ruimte geprojecteerd kunnen worden en dat het niet afhangt van de set van eenheidsvectoren of van coördinatenstelsel. Nu gaan we over naar de parameters. Uit het resultaat van het eenvoudigste artificieel neuraal netwerk uit de § Inleiding tot ANN’s halen we de parameters die nodig zijn voor het berekenen van de uitvoer-node voor de uitkomst setosa: \\[\\mathbf{\\theta_{setosa}^T}=\\begin{bmatrix} 0.773&amp;-0.374&amp;0.571 \\\\ \\end{bmatrix}\\] Dit noemen we een covector of lineaire functionaal. Stelling B.2 Het typeert een covector om niet uitgedrukt te kunnen worden in bepaalde eenheden. Covectoren zijn dus eenheidsloos en dienen als lineaire afbeelding (eng: linear map) om een vector te transformeren naar een andere vector. Het is de veralgemening van deze in elkaar transformeerbare vectoren die met tensoren noemt en de term “tensorflow” komt voort uit het het herhaaldelijk moeten uitvoeren van deze transformaties. Er rest ons nu alleen maar de matrix-vermenigvuldiging uit te voeren. Dit doen volgens het schema in Figuur B.1. Deze figuur laat meteen de veralgemeende situatie zien waarbij meerdere parameter vectoren en meerdere instanties betrokken zijn. Inderdaad, alle waarden voor \\(z\\) binnen eenzelfde laag kunnen doormiddel van één bewerking tegelijk berekend worden. Figuur B.1: Matrix-vermenigvuldiging van. De vermenigvuldiging van twee matrices kan kan dus enkel indien het aantal kolommen van de eerste matrix overeenkomt met het aantal rijen van de tweede. (gebaseerd op deze afbeeldingr) Nu kunnen we terug de wiskundige wereld verlaten en de datawetenschapper wereld betreden, en kunnen de dat kantelen (transponeren) van de matrices vergeten. In R voer je een matrix-vermenigvuldiging uit door middel van de %*% operator: set.seed(42) x &lt;- rnorm(15, 10, 3) %&gt;% matrix(5, 3) theta &lt;- runif(6) %&gt;% matrix(3, 2) z &lt;- x %*% theta \\[z=\\mathbf{x}\\cdot\\mathbf{\\theta}=\\begin{bmatrix} 14.1&amp;9.7&amp;13.9 \\\\ 8.3&amp;14.5&amp;16.9 \\\\ 11.1&amp;9.7&amp;5.8 \\\\ 11.9&amp;16.1&amp;9.2 \\\\ 11.2&amp;9.8&amp;9.6 \\\\ \\end{bmatrix}\\cdot\\begin{bmatrix} 0.7&amp;0.7 \\\\ 0.8&amp;0.0 \\\\ 0.4&amp;0.8 \\\\ \\end{bmatrix}=\\begin{bmatrix} 1.4&amp;2.0 \\\\ 1.7&amp;1.5 \\\\ -0.3&amp;-0.9 \\\\ 2.0&amp;0.2 \\\\ 0.2&amp;0.2 \\\\ \\end{bmatrix}\\] Oefening B.1 Probeer nu zelf voor het eerste neuraal netwerk uit § Inleiding tot ANN’s de resultaten in nn$net.result te bekomen door de invoergegevens te vermenigvuldigen met de parameters in nn$weights. "],
["computations-using-gpu.html", "C Computations using GPU", " C Computations using GPU Computations using machines are typically being carried out using some kind of physical processing units. On many systems there are at least two types of processing units that can perform computations: CPUs (central processing units) and GPUs (graphics processing unit), the latter being originally employed for manipulation of images and for game development. While their constituent parts are similar, their architectures differ greatly: Figuur C.1: The high-level differences between CPUs and GPUs. In a CPU, the control and the cache are governed centrally in the number of arithmetic logic units (ALUs) is limited. In a GPU, control and cache are more distributed and there are many more ALUs. As a consequence, CPUs are more versatile, ‘smarter’ and faster and work best for singular but complex computations. GPUs, on the other hand, are better equipped to handle massive parallel tasks consisting of repetitive but simple computations. "],
["installation.html", "D Installation D.1 Installing CUDA (optional) D.2 The R language D.3 Python D.4 RStudio D.5 Installing Tensorflow D.6 Installation steps that worked for the author D.7 Waar vind ik hulp D.8 Waar vind ik leeralgoritmen terugvinden", " D Installation D.1 Installing CUDA (optional) In 2007 NVIDIA released CUDA® (Compute Unified Device Architecture). CUDA is a parallel computing platform and programming model that enables dramatic increases in computing performance by harnessing the power of the GPU. You can download the platform from here, but before you install it, check that your system is compatible and read the disclaimers provided. For Windows operating systems you can find additional information here. Mind that the above is optional. You can run all the material provided in this course without the GPU computation option. If you plan to install CUDA anyway, make sure to check out the detailed installation steps that worked for the author first. D.2 The R language R is a turing-complete programming language focusing on specifically on data analysis. It is the most popular among statisticians and data scientists and in comparison to Python it is often said to be the brains whereas Python is said to be the muscles (see Upadhyay (2016)): Figuur D.1: In an article on his channel called YOU CANalytics, Roopam Upadhyay has compared R with Batman and Python with Superman. Such a comparison is deemed to be opinionated, and I believe that some of the differences explained in the article are not unconditionally valid, but at least it debunks some of the long held beliefs about the R language. For Windows systems, you can download the latest stable R version from here. It comes with a number of very useful functions already in thebase namespace Again, make sure to continue reading first to check the installation steps that worked for the author below. D.3 Python Python is a general-purpose programming language developed by Guido Van Rossum. The previous paragraph already discusses the differences between R and Python. In addition, some resources or platforms will rely only on one of these two languages. For these reasons, it pays of for a prospective data-scientist to manage both. Persoonlijkheid D.1 (Guido Van Rossum) Guido Van Rossum is the father of Python. He developed the language in 1989 as a Dutch IT-specialist associated with the Mathematisch Centrum in Amsterdam, Netherlands. Originally meant as a low-threshold language for beginning programmers, it now trumps all other. See for yourself on Google trends. D.4 RStudio Rstudio was found by far to be the most valuable and student-friendly IDE (integrated development environment) for working professionally with data, not just in R, but also in Python, Julia and other (scripting) languages such as SQL and Stan. In addition, it is the easiest IDE for people who have limited computer experience. Installing RStudio is free for academic use. See https://rstudio.com for more information, installation generally takes only a few minutes. Once installed, get acquainted with the IDE. See the book of Campbell for details on the IDE (Campbell (2019)). D.5 Installing Tensorflow Around 2005, Google released Tensorflow®. TensorFlow is an open-source Python library to aid with the development, training, building and releasing of machine learning models, particularly deep learning models. To install TensorFlow for Rstudio, visit https://tensorflow.rstudio.com/ and read the instructions there. Check the installation steps used by the author below before you start. D.6 Installation steps that worked for the author Every system is different, so the steps below might not work for you. With these and with the links provided above, you should be able to get up and running in no time. Here is the set of instructions that worked for me, starting from scratch on a Dell XPS 15 7590 with a Windows 10 Pro operating system. For Windows users, remember check the ‘set Path’ option during installation, if available, or set the path manually using the [command line]](https://www.windows-commandline.com/set-path-command-line/). Also, test each step before you go to the next. (optional) Install CUDA version 10.2.89_441.22 for Windows and test with nvcc -V anywhere at the command line prompt Install R and test by running the command R --version anywhere at the command line prompt Install Python 3 and test by running the command python --version anywhere at the command line prompt Install Rstudio and check that the correct version of R is being linked to Install Miniconda in a directory that contains no spaces (not automatically adding to Windows PATH variable as advised, but manually) and test by running the command conda --version anywhere at the command line prompt Add a python environment, here called tf (in the Scripts sub-folder of Miniconda installation folder), read and follow the advise given (if any) and test afterwards with the command conda info --envs or conda env list: conda create -n tf Collecting package metadata (current_repodata.json): done Solving environment: done ==> WARNING: A newer version of conda exists. Open RStudio and, using the console, install the helper package tensorflow that guides the installation: install.packages(&quot;tensorflow&quot;) Installing package into […] (as ‘lib’ is unspecified) also installing the dependencies ‘rappdirs’, ‘config’, ‘reticulate’, ‘tfruns’ trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.6/rappdirs_0.3.1.zip' Content type 'application/zip' length 87285 bytes (85 KB) downloaded 85 KB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.6/config_0.3.zip' Content type 'application/zip' length 27334 bytes (26 KB) downloaded 26 KB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.6/reticulate_1.16.zip' Content type 'application/zip' length 1742735 bytes (1.7 MB) downloaded 1.7 MB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.6/tfruns_1.4.zip' Content type 'application/zip' length 1479931 bytes (1.4 MB) downloaded 1.4 MB trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.6/tensorflow_2.2.0.zip' Content type 'application/zip' length 145036 bytes (141 KB) downloaded 141 KB package ‘rappdirs’ successfully unpacked and MD5 sums checked package ‘config’ successfully unpacked and MD5 sums checked package ‘reticulate’ successfully unpacked and MD5 sums checked package ‘tfruns’ successfully unpacked and MD5 sums checked package ‘tensorflow’ successfully unpacked and MD5 sums checked The downloaded binary packages are in […] Still in RStudios console window, load the tensorflow package in memory (change tf if use you another environment name): install_tensorflow(version = &quot;nightly-gpu&quot;, envname = &quot;tf&quot;) Collecting package metadata (current_repodata.json): ...working... done Solving environment: ...working... done Package Plan ## environment location: […]\\envs\\tf added / updated specs: - python=3.6 The following packages will be downloaded: package | build ---------------------------|----------------- certifi-2020.6.20 | py36h9f0ad1d_0 152 KB conda-forge pip-20.2.2 | py_0 1.1 MB conda-forge python-3.6.11 |h6f26aa1_2_cpython 18.9 MB conda-forge python_abi-3.6 | 1_cp36m 4 KB conda-forge setuptools-49.6.0 | py36h9f0ad1d_0 919 KB conda-forge vc-14.1 | h869be7e_1 6 KB conda-forge vs2015_runtime-14.16.27012 | h30e32a0_2 2.2 MB conda-forge wheel-0.35.1 | pyh9f0ad1d_0 29 KB conda-forge wincertstore-0.2 | py36_1003 13 KB conda-forge ------------------------------------------------------------ Total: 23.3 MB The following NEW packages will be INSTALLED: certifi conda-forge/win-64::certifi-2020.6.20-py36h9f0ad1d_0 pip conda-forge/noarch::pip-20.2.2-py_0 python conda-forge/win-64::python-3.6.11-h6f26aa1_2_cpython python_abi conda-forge/win-64::python_abi-3.6-1_cp36m setuptools conda-forge/win-64::setuptools-49.6.0-py36h9f0ad1d_0 vc conda-forge/win-64::vc-14.1-h869be7e_1 vs2015_runtime conda-forge/win-64::vs2015_runtime-14.16.27012-h30e32a0_2 wheel conda-forge/noarch::wheel-0.35.1-pyh9f0ad1d_0 wincertstore conda-forge/win-64::wincertstore-0.2-py36_1003 […] Collecting tf-nightly-gpu Downloading tf_nightly_gpu-2.4.0.dev20200822-cp36-cp36m-win_amd64.whl (282.9 MB) […] Installation complete. Warning messages: 1: In normalizePath(path.expand(path), winslash, mustWork) : path[1]=\"[…]\\Miniconda\\envs\\tf/python.exe\": Het systeem kan het opgegeven bestand niet vinden 2: In normalizePath(path.expand(path), winslash, mustWork) : path[1]=\"[…]\\Miniconda\\envs\\tf/python.exe\": Het systeem kan het opgegeven bestand niet vinden Restarting R session... NVIDIA’s platforms and application frameworks enable developers to build a wide array of AI applications. Consider potential algorithmic bias when choosing or creating the models being deployed. Work with the model’s developer to ensure that it meets the requirements for the relevant industry and use case; that the necessary instruction and documentation are provided to understand error rates, confidence intervals, and results; and that the model is being used under the conditions and in the manner intended. Test tensorflow using the command below import numpy as np import tensorflow as tf tf.random.set_seed(1) model = tf.keras.Sequential([tf.keras.layers.Dense( \\ units = 1, input_shape = [1])], ) print(tf.reduce_sum(tf.random.normal([1000, 1000]))) 2020-09-17 15:06:29.340924: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll 2020-09-17 15:06:29.365342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5 coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s 2020-09-17 15:06:29.365627: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 2020-09-17 15:06:29.368816: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll 2020-09-17 15:06:29.371298: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll 2020-09-17 15:06:29.372145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll 2020-09-17 15:06:29.375605: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll 2020-09-17 15:06:29.377715: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll 2020-09-17 15:06:29.385621: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll 2020-09-17 15:06:29.385907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0 2020-09-17 15:06:29.386607: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-17 15:06:29.489348: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20dd8509930 initialized for platform Host (this does not guarantee that XLA will be used). Devices: 2020-09-17 15:06:29.489640: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version 2020-09-17 15:06:29.490043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5 coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s 2020-09-17 15:06:29.490402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 2020-09-17 15:06:29.490567: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll 2020-09-17 15:06:29.490775: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll 2020-09-17 15:06:29.491080: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll 2020-09-17 15:06:29.491298: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll 2020-09-17 15:06:29.491542: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll 2020-09-17 15:06:29.491784: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll 2020-09-17 15:06:29.492628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0 2020-09-17 15:06:30.261351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix: 2020-09-17 15:06:30.261492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263] 0 2020-09-17 15:06:30.261571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0: N 2020-09-17 15:06:30.261943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2907 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5) 2020-09-17 15:06:30.289124: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20e0e74be40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 2020-09-17 15:06:30.289298: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): GeForce GTX 1650, Compute Capability 7.5 tf.Tensor(22.580101, shape=(), dtype=float32) D.7 Waar vind ik hulp Als je een bepaalde actie wil uitvoeren in R, maar je kent de naam van het pakket niet, dan zijn R Bloggers en Stack Overflow vermoedelijk de beste plaatsen om te zoeken. Gaat over de RStudio IDE, dan is de RStudio Community misschien wel iets. Zoek je toch op Google, voeg dan de term package toe aan de lijst van zoektermen om betere resultaten te bekomen. Heb je de naam van een R pakket of een functie en werk je met RStudio, dan moet je meestal de IDE niet te verlaten. Wil je meer weten over een bepaald pakket, dan zijn er vignettes (soort handleiding) en documentatie. Vignettes roep je aan d.m.v. de vignette() functie, documentatie met ? of help(). Verder heeft R ook demo’s en kan je rechtstreeks de voorbeelden uit de technische documentatie uitvoeren zonder de code te moeten kopiëren. # Start help.start() # Documentatie ?magrittr ?magrittr::extract # Zoeken naar hulp op basis van term help.search(&quot;pipes&quot;) # Informatie/handleiding specifiek pakket vignette(&#39;magrittr&#39;) # Lijst pakketten van gekoppelde bibliotheken vignette(all = FALSE) # Lijst pakketten geïnstalleerde bibliotheken vignette(all = TRUE) # Lijst alle vignettes browseVignettes() # Demonstratie demo(image) # Voorbeeld-code uit documentatie example(&quot;stl&quot;) # Pakket info (auteur, versie, ...) library(help = &quot;magrittr&quot;) # R site RSiteSearch(&quot;magrittr&quot;) # CRAN repository pakketten browseURL(&quot;https://cran.r-project.org/web/packages/available_packages_by_name.html&quot;) # CRAN repo specifiek pakket browseURL(&quot;https://cran.r-project.org/web/packages/magrittr/index.html&quot;) # CRAN task views browseURL(&quot;https://cran.r-project.org/web/views/&quot;) # CRAN speciefieke task view browseURL(&quot;https://cran.r-project.org/web/views/Hydrology.html&quot;) Voor Python en andere talen verwijs ik naar Stack Overflow. Zie ook volgende paragraaf voor wat meer info over rond Python. D.8 Waar vind ik leeralgoritmen terugvinden Laten we beginnen met waar je de verscheidene leeralgoritmen kunt terugvinden. In sommige gevallen, zoals voor een enkelvoudig perceptron, kan je het zelf gewoon coderen maar meestal worden de algoritmen gelukkig ter beschikking gesteld. Omdat wiskundigen eerder R gebruiken zullen de nieuwere leeralgoritmen gewoonlijk eerst in R verschijnen en pas later in Python. Bij het publiceren van nieuwe methoden wordt er immers vaak van de auteurs verwacht dat ze het algoritme als een peer-reviewed R pakket (eng: package) registreren en publiek ter beschikking stellen. Het Comprehensive R Archive Network (CRAN), dat de R pakketten beheert en erop toeziet dat de pakketten zowel wiskundig als programmatorisch aan bepaalde standaarden voldoet, stelt zogenaamde task views op met telkens een overzicht van de pakketten die kunnen helpen bij een welbepaalde taak. Zoeken we naar “machine learning”, komen we op honderden pakketten uit van ahaz tot xgboost4 en vele van deze pakketten geven toegang tot tientallen verschillende typen leeralgoritmen. Maar dat is niet alles. Op de webpagina met het overzicht van alle task views, vinden we bijvoorbeeld het onderwerp natuurlijke taalverwerking (eng: natural language processing; NLP) met nog eens tientallen pakketten specifiek rond dit thema. Voor Python ontwikkeling verwijzen we voor leeralgoritmen eerst en vooral naar scikit-learn. Deze bibliotheek biedt een ruime gereedschapskist aan voor beginnende datawetenschappers. De naam komt trouwens van Science-kit \\(\\rightarrow\\) scikit \\(\\rightarrow\\) sk. Dan is er Tensorflow waar in de cursus dieper op wordt ingegaan. Ook hier vind je een rijke schat aan informatie. Python biedt een aantal voordelen voor ML en het belangrijkste voordeel is misschien wel dat Python een algemene programmeertaal is waarmee je gemakkelijker volwaardige applicaties kunt maken of met hardware interfacen. De bewering dat Python ‘krachtiger’ of ‘performanter’ zou zijn dan R neem ik eerder met een korreltje zout. Op de weinige momenten waarop beide platformen op eerlijke wijze vergeleken worden is het verschil onbeduidend en bovendien moet je voornamelijk rekening houden met ontwikkeltijd en daar biedt Python zeker geen voordeel. Tenslotte vermeld ik hier nog even Julia, een opkomende taal waarin ML eenvoudiger moet worden. Bronvermelding "],
["git.html", "E Git", " E Git Git is een populair maar spijtig genoeg niet al te gebruiksvriendelijk versiebeheersysteem en tool om het werken in team-verband te stimuleren. In combinatie met github komt het hierop neer: include_graphics(&quot;img/git.svg&quot;) Figuur E.1: De voornaamste (tijdelijke) opslag plaatsen zoals gedefinieerd door het git versiebeheersysteem met de belangrijkste acties die uitgevoerd kunnen en die te maken hebben met het verplaatsen van data van de ene naar de andere (tijdelijke) opslag. Typisch zal de remote master GitHub zijn. Je kan git vanuit de opdracht-prompt aansturen of vanuit een gebruikersinterface (UI), zie bijv. deze link hiervoor. Git is ook ingebakken in RStudio, dus een extra UI is in feite niet nodig. De voorbeelden hieronder zijn voor de opdracht-prompt. Van daaruit is het gewoonlijk maar een kleine stap om in een UI de gelijkaardige commando’s terug te vinden. Denk eraan dat de folder waarin je de opdrachten uitvoert moet overeenkomen met de werk-folder. Controle installatie &gt; git --version git version 2.28.0.windows.1 Authenticatie &gt; git config --global user.name &quot;usernamehere&quot; &gt; git config --global user.email &quot;email@provider.com&quot; user.name=usernamehere user.email=email@provider.com credential.helper=store Van bestaande lokale folder, al dan niet met bestanden in, een lokale repository maken die je misschien later nog naar een remote repo kunt opladen. &gt; cd de_bestaande_lokale_folder &gt; git init &gt; git add . &gt; git status # Controle &gt; git commit -m &quot;Eerste commit&quot; Initialized empty Git repository in de_bestaande_lokale_folder/.git/ On branch master No commits yet Changes to be committed: (use \"git rm --cached ...\" to unstage) new file: tmp.txt [master (root-commit) 845dafd] Eerste commit 1 file changed, 1 insertion(+) create mode 100644 tmp.txt Van een GitHub remote repository een lokale repository maken met lokale werk-folder. Eerst ga je naar GitHub, zoek je de repository en kopieer je de bijhorende link: include_graphics(&quot;img/github-clone.png&quot;) en daarna geef je de volgende opdrachten in (vervang de repo naam). Let erop dat de remote repo als subfolder wordt aangemaakt van de folder waarin je de opdracht geeft!: &gt; git clone https://github.com/ddhaese/Tetris.git &gt; cd tetris &gt; git remote -v Cloning into 'Tetris'... remote: Enumerating objects: 18, done. remote: Counting objects: 100% (18/18), done. remote: Compressing objects: 100% (16/16), done. remote: Total 30 (delta 4), reused 16 (delta 2), pack-reused 12 Unpacking objects: 100% (30/30), 1.13 MiB | 1.78 MiB/s, done. origin https://github.com/ddhaese/Tetris.git (fetch) origin https://github.com/ddhaese/Tetris.git (push) Een bestaande lokale repo koppelen aan een remote GitHub repo &gt; git remote add origin remote_repository_URL Wijzigingen doorvoeren, eerst naar lokale repo, dan naar remote repo. Nadat de wijzigingen zijn aangebracht lokaal:. &gt; git status &gt; git add tmp.txt &gt; git status &gt; git commit -m &quot;wijziging&quot; &gt; git push -u origin master On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git restore ...\" to discard changes in working directory) modified: tmp.txt no changes added to commit (use \"git add\" and/or \"git commit -a\") On branch master Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git restore ...\" to discard changes in working directory) modified: tmp.txt no changes added to commit (use \"git add\" and/or \"git commit -a\") [master c17bbe1] wijziging 1 file changed, 1 insertion(+), 1 deletion(-) Enumerating objects: 5, done. Counting objects: 100% (5/5), done. Delta compression using up to 16 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (3/3), 303 bytes | 303.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To remote_repository_URL cc654bf..ee026e2 master -> master Branch 'master' set up to track remote branch 'master' from 'origin'. Voor andere opdrachten verwijs ik naar Udacity’s Version Control with Git. "],
["antwoorden.html", "F Antwoorden F.1 Hoofdstuk 4", " F Antwoorden F.1 Hoofdstuk 4 Oefening 4.2. library(beeswarm) library(vioplot) rhDNase &lt;- rhDNase %&gt;% as.data.table bduur_log &lt;- rhDNase[, log(ivstop - ivstart)] bduur_log &lt;- bduur_log[!is.na(bduur_log) &amp; !is.infinite(bduur_log)] par(mfrow = c(2, 2), pch = 19, mar = rep(1, 4), xaxt = &quot;n&quot;) bduur_log %&gt;% dotchart(pch = 19, lcolor = NA) bduur_log %&gt;% boxplot(horizontal = TRUE) bduur_log %&gt;% beeswarm(horizontal = TRUE) bduur_log %&gt;% vioplot(horizontal = TRUE) "],
["bronvermelding.html", "Bronvermelding", " Bronvermelding "]
]
